{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "credit to https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CSY\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19579, 3), (8392, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_null = train.isnull().sum()\n",
    "check_null[check_null!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "author_cnt = train.author.value_counts()\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "sns.barplot(author_cnt.index, author_cnt.values)\n",
    "plt.ylabel('Number of paragraph', fontsize=12)\n",
    "plt.xlabel('Author', fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "ps = nltk.PorterStemmer()\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text = \" \".join([ps.stem(word) for word in tokens if word not in stopwords])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    process howev afford mean ascertain dimens dun...\n",
       "1                  never occur fumbl might mere mistak\n",
       "2    left hand gold snuff box caper hill cut manner...\n",
       "3    love spring look windsor terrac sixteen fertil...\n",
       "4    find noth els even gold superintend abandon at...\n",
       "Name: clean_text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"clean_text\"] = train.text.apply(lambda x: clean_text(x))\n",
    "train[\"clean_text\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>still urg leav ireland inquietud impati father...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>fire want fan could readili fan newspap govern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "      <td>broken frail door found two cleanli pick human...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>While I was thinking how I should possibly man...</td>\n",
       "      <td>think possibl manag without one actual tumbl h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>I am not sure to what limit his knowledge may ...</td>\n",
       "      <td>sure limit knowledg may extend</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...   \n",
       "1  id24541  If a fire wanted fanning, it could readily be ...   \n",
       "2  id00134  And when they had broken down the frail door t...   \n",
       "3  id27757  While I was thinking how I should possibly man...   \n",
       "4  id04081  I am not sure to what limit his knowledge may ...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  still urg leav ireland inquietud impati father...  \n",
       "1  fire want fan could readili fan newspap govern...  \n",
       "2  broken frail door found two cleanli pick human...  \n",
       "3  think possibl manag without one actual tumbl h...  \n",
       "4                     sure limit knowledg may extend  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"clean_text\"] = test.text.apply(lambda x: clean_text(x))\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "import string\n",
    "## Number of words in the text ##\n",
    "train[\"num_words\"] = train[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "test[\"num_words\"] = test[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "## Number of unique words in the text ##\n",
    "train[\"num_unique_words\"] = train[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "test[\"num_unique_words\"] = test[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "## Number of stopwords in the text ##\n",
    "train[\"num_stopwords\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "test[\"num_stopwords\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "\n",
    "## Number of punctuations in the text ##\n",
    "train[\"num_punctuations\"] =train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "test[\"num_punctuations\"] =test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct_pct2(text, punc):\n",
    "    count = sum([1 for char in text if char==punc])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>comma%</th>\n",
       "      <th>semicolon%</th>\n",
       "      <th>colon%</th>\n",
       "      <th>question%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>process howev afford mean ascertain dimens dun...</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>never occur fumbl might mere mistak</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>left hand gold snuff box caper hill cut manner...</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>love spring look windsor terrac sixteen fertil...</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>find noth els even gold superintend abandon at...</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "                                          clean_text  num_words  \\\n",
       "0  process howev afford mean ascertain dimens dun...         41   \n",
       "1                never occur fumbl might mere mistak         14   \n",
       "2  left hand gold snuff box caper hill cut manner...         36   \n",
       "3  love spring look windsor terrac sixteen fertil...         34   \n",
       "4  find noth els even gold superintend abandon at...         27   \n",
       "\n",
       "   num_unique_words  num_stopwords  num_punctuations  comma%  semicolon%  \\\n",
       "0                35             19                 7     2.1         1.0   \n",
       "1                14              8                 1     0.0         0.0   \n",
       "2                32             16                 5     2.4         0.0   \n",
       "3                32             13                 4     1.7         0.0   \n",
       "4                25             11                 4     1.4         0.7   \n",
       "\n",
       "   colon%  question%  \n",
       "0     0.0        0.0  \n",
       "1     0.0        0.0  \n",
       "2     0.0        0.0  \n",
       "3     0.0        0.0  \n",
       "4     0.0        0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"comma%\"] = train.text.apply(lambda x: punct_pct2(x, ','))\n",
    "train[\"semicolon%\"] = train.text.apply(lambda x: punct_pct2(x, ';'))\n",
    "train[\"colon%\"] = train.text.apply(lambda x: punct_pct2(x, ':'))\n",
    "train[\"question%\"] = train.text.apply(lambda x: punct_pct2(x, '?'))\n",
    "\n",
    "test[\"comma%\"] = test.text.apply(lambda x: punct_pct2(x, ','))\n",
    "test[\"semicolon%\"] = test.text.apply(lambda x: punct_pct2(x, ';'))\n",
    "test[\"colon%\"] = test.text.apply(lambda x: punct_pct2(x, ':'))\n",
    "test[\"question%\"] = test.text.apply(lambda x: punct_pct2(x, '?'))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vars = [\"comma%\", \"semicolon%\", \"question%\", \"colon%\",\n",
    "            \"num_punctuations\", \"num_stopwords\", \"num_unique_words\", \"num_words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import doc2vec\n",
    "\n",
    "def label_sentences(corpus, label_type):\n",
    "    \"\"\"\n",
    "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "    We do this by using the TaggedDocument method. The format will be like \"TRAIN_i\" where \"i\" is\n",
    "    a dummy index of the complaint narrative.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(doc2vec.TaggedDocument(v.split(), [label]))\n",
    "    return labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_Train, x_Val, y_Train, y_Val = train_test_split(train, train.author, random_state=1, test_size=0.2)\n",
    "xTrain = label_sentences(x_Train.clean_text, 'Train')\n",
    "xVal = label_sentences(x_Val.clean_text, 'Val')\n",
    "len(xTrain), len(xVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8392"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTest = label_sentences(test.clean_text, 'Test')\n",
    "len(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['twice', 'waylaid', 'footpad', 'person', 'rigor', 'search', 'inspect'], tags=['Train_0']),\n",
       " TaggedDocument(words=['autumn', 'year', 'spirit', 'emigr', 'crept', 'among', 'survivor', 'congreg', 'variou', 'part', 'england', 'met', 'london'], tags=['Train_1']),\n",
       " TaggedDocument(words=['mention', 'muriton', 'red', 'tongu', 'cauliflow', 'velouté', 'sauc', 'veal', 'à', 'la', 'st', 'menehoult', 'marinad', 'à', 'la', 'st', 'florentin', 'orang', 'jelli', 'en', 'mosäiqu'], tags=['Train_2'])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = xTrain + xVal + xTest\n",
    "all_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"\n",
    "    Get vectors from trained doc2vec model\n",
    "    :param doc2vec_model: Trained Doc2Vec model\n",
    "    :param corpus_size: Size of the data\n",
    "    :param vectors_size: Size of the embedding vectors\n",
    "    :param vectors_type: Training or Testing vectors\n",
    "    :return: list of vectors\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1445828.69it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=2, alpha=0.065, min_alpha=0.05)\n",
    "model_dbow.build_vocab([x for x in tqdm(all_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 854408.84it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 881142.80it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 2088676.62it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1253098.89it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 2169157.39it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1879688.49it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 2168836.58it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1658780.04it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1483903.28it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1944522.52it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1879839.08it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 2013919.68it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1025317.48it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1944393.61it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1484072.22it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1879628.26it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1483884.51it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 2088676.62it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1409659.08it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 762068.21it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1566423.80it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1819151.16it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1819179.36it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 2013815.97it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1174904.13it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 854402.61it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1025209.97it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1342843.64it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1708892.34it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1006864.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 44.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors_dbow = get_vectors(model_dbow, len(xTrain), 300, 'Train')\n",
    "val_vectors_dbow = get_vectors(model_dbow, len(xVal), 300, 'Val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow.save('d2v_dbow.doc2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 881076.63it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.05)\n",
    "model_dm.build_vocab([x for x in tqdm(all_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 794267.55it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1762365.02it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 989331.42it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1253179.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1880080.08it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1483997.13it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1762259.13it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 989381.48it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1611279.58it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1611124.68it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 909568.53it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1281669.26it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 2013815.97it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1105707.45it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1174915.90it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1879929.45it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1566381.98it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1879748.72it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1150918.50it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1342659.22it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 972261.26it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1524195.83it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 696204.88it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 989339.76it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 2013815.97it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 2256045.48it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1762285.60it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 2014230.87it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1944748.15it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 27971/27971 [00:00<00:00, 1445935.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors_dm = get_vectors(model_dm, len(xTrain), 300, 'Train')\n",
    "val_vectors_dm = get_vectors(model_dm, len(xVal), 300, 'Val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dm.save('d2v_dm.doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "model_dm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ensemble two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concat_vectors(model1,model2, corpus_size, vectors_size, vectors_type):\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = np.append(model1.docvecs[prefix],model2.docvecs[prefix])\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_dbow_dm = get_concat_vectors(model_dbow,model_dm, len(xTrain), 600, 'Train')\n",
    "val_vecs_dbow_dm = get_concat_vectors(model_dbow,model_dm, len(xVal), 600, 'Val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.26423790e-02, -1.03586011e-01, -9.34147611e-02, ...,\n",
       "         2.04044889e-04,  1.58772664e-03, -1.10418745e-03],\n",
       "       [ 2.05712512e-01,  4.36340570e-02, -1.41410843e-01, ...,\n",
       "         7.61901319e-04, -1.57967920e-03,  5.10557496e-04],\n",
       "       [ 1.85213044e-01,  8.65301192e-02, -7.81153142e-02, ...,\n",
       "         9.47897148e-04, -1.24921316e-05, -1.54612854e-03],\n",
       "       ...,\n",
       "       [ 1.55942678e-01,  4.78256643e-02, -1.10023409e-01, ...,\n",
       "        -1.22167589e-03,  1.18912058e-03,  8.59428546e-04],\n",
       "       [ 1.14518598e-01,  3.60401385e-02, -1.50026843e-01, ...,\n",
       "         8.80557403e-04,  1.56548433e-03,  4.80017974e-04],\n",
       "       [ 1.11187123e-01,  9.98829380e-02,  7.99867362e-02, ...,\n",
       "        -1.21243251e-03, -8.98093509e-04, -4.60802170e-04]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vecs_dbow_dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15663, 600)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vecs_dbow_dm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comma%</th>\n",
       "      <th>semicolon%</th>\n",
       "      <th>question%</th>\n",
       "      <th>colon%</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comma%  semicolon%  question%  colon%  num_punctuations  num_stopwords  \\\n",
       "0     2.2         0.0        0.0     0.0                 5             10   \n",
       "1     3.2         0.0        0.0     0.0                 5             12   \n",
       "\n",
       "   num_unique_words  num_words  \n",
       "0                18         18  \n",
       "1                22         27  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vars = x_Train[num_vars].reset_index(drop = True)\n",
    "train_vars.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comma%</th>\n",
       "      <th>semicolon%</th>\n",
       "      <th>question%</th>\n",
       "      <th>colon%</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>31</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comma%  semicolon%  question%  colon%  num_punctuations  num_stopwords  \\\n",
       "0     4.9         0.0        2.4     0.0                 3              4   \n",
       "1     1.8         1.2        0.0     0.0                 6             20   \n",
       "\n",
       "   num_unique_words  num_words  \n",
       "0                 9          9  \n",
       "1                31         36  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_vars = x_Val[num_vars].reset_index(drop = True)\n",
    "val_vars.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15663, 608), (3916, 608))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_all = pd.concat([pd.DataFrame(train_vecs_dbow_dm), train_vars], axis=1)\n",
    "xval_all = pd.concat([pd.DataFrame(val_vecs_dbow_dm), val_vars], axis=1)\n",
    "xtrain_all.shape, xval_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CSY\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\CSY\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "C:\\Users\\CSY\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scale the data before any neural net:\n",
    "scl = StandardScaler()\n",
    "xtrain_scl = scl.fit_transform(xtrain_all)\n",
    "xval_scl = scl.transform(xval_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15663, 608), (15663, 3), (3916, 608), (3916, 3))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing, decomposition, model_selection, metrics\n",
    "enc = preprocessing.LabelEncoder()\n",
    "ytrain_enc = enc.fit_transform(y_Train.values)\n",
    "yvalid_enc = enc.fit_transform(y_Val.values)\n",
    "\n",
    "from keras.utils import np_utils\n",
    "# we need to binarize the labels for the neural net\n",
    "ytrain_enc = np_utils.to_categorical(ytrain_enc)\n",
    "yvalid_enc = np_utils.to_categorical(yvalid_enc)\n",
    "\n",
    "xtrain_scl.shape, ytrain_enc.shape, xval_scl.shape, yvalid_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def custom_loss(eps):\n",
    "    def lossfunc(y_true, y_pred):\n",
    "        \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "        :param actual: Array containing the actual target classes\n",
    "        :param predicted: Matrix with class predictions, one probability per class\n",
    "        \"\"\"\n",
    "        actual = y_true\n",
    "        predicted = y_pred\n",
    "        # Convert 'actual' to a binary array if it's not already:\n",
    "        if len(actual.shape) == 1:\n",
    "            actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "            for i, val in enumerate(actual):\n",
    "                actual2[i, val] = 1\n",
    "            actual = actual2\n",
    "        \n",
    "        clip = np.clip(predicted, eps, 1 - eps)\n",
    "        rows = actual.shape[0]\n",
    "        vsota = tf.sum(actual * np.log(clip))\n",
    "        return -1.0 / rows * vsota\n",
    "    return lossfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\CSY\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\CSY\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\CSY\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 3.3760 - val_loss: 1.0732\n",
      "Epoch 2/100\n",
      " - 2s - loss: 1.5861 - val_loss: 1.0863\n",
      "Epoch 3/100\n",
      " - 2s - loss: 1.2635 - val_loss: 1.0882\n",
      "Epoch 4/100\n",
      " - 2s - loss: 1.1553 - val_loss: 1.0857\n",
      "Epoch 5/100\n",
      " - 2s - loss: 1.0918 - val_loss: 1.0773\n",
      "Epoch 6/100\n",
      " - 2s - loss: 1.0491 - val_loss: 1.0542\n",
      "Epoch 7/100\n",
      " - 2s - loss: 0.9963 - val_loss: 0.9847\n",
      "Epoch 8/100\n",
      " - 2s - loss: 0.9153 - val_loss: 0.8819\n",
      "Epoch 9/100\n",
      " - 2s - loss: 0.8417 - val_loss: 0.8107\n",
      "Epoch 10/100\n",
      " - 2s - loss: 0.7829 - val_loss: 0.7438\n",
      "Epoch 11/100\n",
      " - 2s - loss: 0.7456 - val_loss: 0.7014\n",
      "Epoch 12/100\n",
      " - 2s - loss: 0.7136 - val_loss: 0.6788\n",
      "Epoch 13/100\n",
      " - 2s - loss: 0.6773 - val_loss: 0.6466\n",
      "Epoch 14/100\n",
      " - 2s - loss: 0.6697 - val_loss: 0.6386\n",
      "Epoch 15/100\n",
      " - 2s - loss: 0.6560 - val_loss: 0.6479\n",
      "Epoch 16/100\n",
      " - 2s - loss: 0.6368 - val_loss: 0.6205\n",
      "Epoch 17/100\n",
      " - 2s - loss: 0.6133 - val_loss: 0.6044\n",
      "Epoch 18/100\n",
      " - 2s - loss: 0.6096 - val_loss: 0.6012\n",
      "Epoch 19/100\n",
      " - 2s - loss: 0.5988 - val_loss: 0.6006\n",
      "Epoch 20/100\n",
      " - 2s - loss: 0.5974 - val_loss: 0.5977\n",
      "Epoch 21/100\n",
      " - 2s - loss: 0.5854 - val_loss: 0.5867\n",
      "Epoch 22/100\n",
      " - 2s - loss: 0.5829 - val_loss: 0.5967\n",
      "Epoch 23/100\n",
      " - 2s - loss: 0.5639 - val_loss: 0.5853\n",
      "Epoch 24/100\n",
      " - 2s - loss: 0.5651 - val_loss: 0.5785\n",
      "Epoch 25/100\n",
      " - 2s - loss: 0.5582 - val_loss: 0.5776\n",
      "Epoch 26/100\n",
      " - 2s - loss: 0.5476 - val_loss: 0.5763\n",
      "Epoch 27/100\n",
      " - 2s - loss: 0.5445 - val_loss: 0.5701\n",
      "Epoch 28/100\n",
      " - 2s - loss: 0.5425 - val_loss: 0.5786\n",
      "Epoch 29/100\n",
      " - 2s - loss: 0.5408 - val_loss: 0.5725\n",
      "Epoch 30/100\n",
      " - 2s - loss: 0.5229 - val_loss: 0.5690\n",
      "Epoch 31/100\n",
      " - 2s - loss: 0.5183 - val_loss: 0.5614\n",
      "Epoch 32/100\n",
      " - 2s - loss: 0.5172 - val_loss: 0.5630\n",
      "Epoch 33/100\n",
      " - 2s - loss: 0.5216 - val_loss: 0.5673\n",
      "Epoch 34/100\n",
      " - 2s - loss: 0.5083 - val_loss: 0.5699\n",
      "Epoch 35/100\n",
      " - 2s - loss: 0.5057 - val_loss: 0.5585\n",
      "Epoch 36/100\n",
      " - 2s - loss: 0.5027 - val_loss: 0.5594\n",
      "Epoch 37/100\n",
      " - 2s - loss: 0.4984 - val_loss: 0.5637\n",
      "Epoch 38/100\n",
      " - 2s - loss: 0.4990 - val_loss: 0.5605\n",
      "Epoch 39/100\n",
      " - 2s - loss: 0.4858 - val_loss: 0.5451\n",
      "Epoch 40/100\n",
      " - 2s - loss: 0.4788 - val_loss: 0.5575\n",
      "Epoch 41/100\n",
      " - 2s - loss: 0.4699 - val_loss: 0.5427\n",
      "Epoch 42/100\n",
      " - 2s - loss: 0.4732 - val_loss: 0.5527\n",
      "Epoch 43/100\n",
      " - 2s - loss: 0.4732 - val_loss: 0.5535\n",
      "Epoch 44/100\n",
      " - 2s - loss: 0.4550 - val_loss: 0.5429\n",
      "Epoch 45/100\n",
      " - 2s - loss: 0.4610 - val_loss: 0.5400\n",
      "Epoch 46/100\n",
      " - 2s - loss: 0.4574 - val_loss: 0.5464\n",
      "Epoch 47/100\n",
      " - 2s - loss: 0.4547 - val_loss: 0.5484\n",
      "Epoch 48/100\n",
      " - 2s - loss: 0.4367 - val_loss: 0.5469\n",
      "Epoch 49/100\n",
      " - 2s - loss: 0.4477 - val_loss: 0.5447\n",
      "Epoch 50/100\n",
      " - 2s - loss: 0.4340 - val_loss: 0.5443\n",
      "Epoch 51/100\n",
      " - 2s - loss: 0.4267 - val_loss: 0.5402\n",
      "Epoch 52/100\n",
      " - 2s - loss: 0.4236 - val_loss: 0.5418\n",
      "Epoch 53/100\n",
      " - 2s - loss: 0.4259 - val_loss: 0.5387\n",
      "Epoch 54/100\n",
      " - 2s - loss: 0.4131 - val_loss: 0.5368\n",
      "Epoch 55/100\n",
      " - 2s - loss: 0.4081 - val_loss: 0.5429\n",
      "Epoch 56/100\n",
      " - 2s - loss: 0.4101 - val_loss: 0.5419\n",
      "Epoch 57/100\n",
      " - 2s - loss: 0.4015 - val_loss: 0.5485\n",
      "Epoch 58/100\n",
      " - 2s - loss: 0.3983 - val_loss: 0.5478\n",
      "Epoch 59/100\n",
      " - 2s - loss: 0.3973 - val_loss: 0.5441\n",
      "Epoch 60/100\n",
      " - 2s - loss: 0.3935 - val_loss: 0.5435\n",
      "Epoch 61/100\n",
      " - 2s - loss: 0.3814 - val_loss: 0.5413\n",
      "Epoch 62/100\n",
      " - 2s - loss: 0.3931 - val_loss: 0.5499\n",
      "Epoch 63/100\n",
      " - 2s - loss: 0.3778 - val_loss: 0.5450\n",
      "Epoch 64/100\n",
      " - 2s - loss: 0.3770 - val_loss: 0.5467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2560a842710>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "model_d2v = Sequential()\n",
    "model_d2v.add(Dense(256, activation='relu'))\n",
    "model_d2v.add(Dropout(0.7))\n",
    "model_d2v.add(Dense(512, activation='relu'))\n",
    "model_d2v.add(Dropout(0.9))\n",
    "model_d2v.add(Dense(256, activation='relu'))\n",
    "model_d2v.add(Dropout(0.8))\n",
    "\n",
    "model_d2v.add(Dense(3))  # this is the output layer\n",
    "model_d2v.add(Activation('softmax'))\n",
    "model_d2v.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# model_d2v.compile(loss=custom_loss(1e-15), optimizer='adam')\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True) \n",
    "model_d2v.fit(x=xtrain_scl, y=ytrain_enc, validation_data=(xval_scl, yvalid_enc), \n",
    "                    epochs=100, batch_size=256, verbose=2, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8392, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>comma%</th>\n",
       "      <th>semicolon%</th>\n",
       "      <th>colon%</th>\n",
       "      <th>question%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>still urg leav ireland inquietud impati father...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>fire want fan could readili fan newspap govern...</td>\n",
       "      <td>62</td>\n",
       "      <td>49</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...   \n",
       "1  id24541  If a fire wanted fanning, it could readily be ...   \n",
       "\n",
       "                                          clean_text  num_words  \\\n",
       "0  still urg leav ireland inquietud impati father...         19   \n",
       "1  fire want fan could readili fan newspap govern...         62   \n",
       "\n",
       "   num_unique_words  num_stopwords  num_punctuations  comma%  semicolon%  \\\n",
       "0                19              9                 3     2.2         0.0   \n",
       "1                49             33                 7     2.2         0.0   \n",
       "\n",
       "   colon%  question%  \n",
       "0     0.0        0.0  \n",
       "1     0.0        0.0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test.shape)\n",
    "test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CSY\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.4744990e-01, 2.4017558e-02, 8.2853246e-01],\n",
       "       [7.4536675e-01, 1.6391975e-01, 9.0713501e-02],\n",
       "       [1.9660375e-01, 7.7289927e-01, 3.0496996e-02],\n",
       "       ...,\n",
       "       [4.7911134e-01, 3.7519179e-02, 4.8336956e-01],\n",
       "       [1.4604101e-01, 1.3087759e-02, 8.4087121e-01],\n",
       "       [3.5947759e-02, 9.6333349e-01, 7.1876281e-04]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vectors_dbow_dm = get_concat_vectors(model_dbow,model_dm,len(xTest), 600, 'Test')\n",
    "test_vars = test[num_vars].reset_index(drop = True)\n",
    "xtest_all = pd.concat([pd.DataFrame(test_vectors_dbow_dm), test_vars], axis=1)\n",
    "xtest_scl = scl.transform(xtest_all)\n",
    "predictions = model_d2v.predict_proba(xtest_scl)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8392, 3)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_df = pd.concat([test.id, pd.DataFrame(predictions)], axis=1)\n",
    "nn_df.columns=[\"id\",\"EAP\",\"HPL\",\"MWS\"]\n",
    "nn_df.to_csv(\"nn.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
