{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "credit to:\n",
    " - https://www.kaggle.com/knowledgegrappler/embeddings-features-tdf-idf-let-s-party/comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  EAP  HPL  MWS\n",
       "0  id26305  This process, however, afforded me no means of...    1    0    0\n",
       "1  id17569  It never once occurred to me that the fumbling...    0    1    0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "#CREATE TARGET VARIABLE\n",
    "train[\"EAP\"] = (train.author==\"EAP\")*1\n",
    "train[\"HPL\"] = (train.author==\"HPL\")*1\n",
    "train[\"MWS\"] = (train.author==\"MWS\")*1\n",
    "train.drop(\"author\", 1, inplace=True)\n",
    "target_vars = [\"EAP\", \"HPL\", \"MWS\"]\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "import string\n",
    "## Number of words in the text ##\n",
    "train[\"num_words\"] = train[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "test[\"num_words\"] = test[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "## Number of unique words in the text ##\n",
    "train[\"num_unique_words\"] = train[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "test[\"num_unique_words\"] = test[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "## Number of stopwords in the text ##\n",
    "train[\"num_stopwords\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "test[\"num_stopwords\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "\n",
    "## Number of punctuations in the text ##\n",
    "train[\"num_punctuations\"] =train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "test[\"num_punctuations\"] =test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct_pct2(text, punc):\n",
    "    count = sum([1 for char in text if char==punc])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>stem_text</th>\n",
       "      <th>seq_text_stem</th>\n",
       "      <th>comma%</th>\n",
       "      <th>semicolon%</th>\n",
       "      <th>colon%</th>\n",
       "      <th>question%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4.658537</td>\n",
       "      <td>this process  howev  afford me no mean of asce...</td>\n",
       "      <td>[27, 1876, 161, 743, 22, 37, 201, 2, 1652, 1, ...</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.142857</td>\n",
       "      <td>it never onc occur to me that the fumbl might ...</td>\n",
       "      <td>[10, 99, 138, 672, 4, 22, 9, 1, 3675, 85, 23, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.583333</td>\n",
       "      <td>in his left hand was a gold snuff box  from wh...</td>\n",
       "      <td>[7, 15, 164, 122, 8, 6, 935, 4166, 636, 24, 18...</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.088235</td>\n",
       "      <td>how love is spring as we look from windsor ter...</td>\n",
       "      <td>[133, 106, 26, 749, 16, 35, 94, 24, 903, 2393,...</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.481481</td>\n",
       "      <td>find noth els  not even gold  the superintend ...</td>\n",
       "      <td>[207, 194, 882, 20, 67, 935, 1, 3204, 1281, 15...</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  EAP  HPL  MWS  \\\n",
       "0  id26305  This process, however, afforded me no means of...    1    0    0   \n",
       "1  id17569  It never once occurred to me that the fumbling...    0    1    0   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    1    0    0   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...    0    0    1   \n",
       "4  id12958  Finding nothing else, not even gold, the Super...    0    1    0   \n",
       "\n",
       "   num_words  num_unique_words  num_stopwords  num_punctuations  \\\n",
       "0         41                35             19                 7   \n",
       "1         14                14              8                 1   \n",
       "2         36                32             16                 5   \n",
       "3         34                32             13                 4   \n",
       "4         27                25             11                 4   \n",
       "\n",
       "   num_words_upper  num_words_title  mean_word_len  \\\n",
       "0                2                3       4.658537   \n",
       "1                0                1       4.142857   \n",
       "2                0                1       4.583333   \n",
       "3                0                4       5.088235   \n",
       "4                0                2       5.481481   \n",
       "\n",
       "                                           stem_text  \\\n",
       "0  this process  howev  afford me no mean of asce...   \n",
       "1  it never onc occur to me that the fumbl might ...   \n",
       "2  in his left hand was a gold snuff box  from wh...   \n",
       "3  how love is spring as we look from windsor ter...   \n",
       "4  find noth els  not even gold  the superintend ...   \n",
       "\n",
       "                                       seq_text_stem  comma%  semicolon%  \\\n",
       "0  [27, 1876, 161, 743, 22, 37, 201, 2, 1652, 1, ...     2.1         1.0   \n",
       "1  [10, 99, 138, 672, 4, 22, 9, 1, 3675, 85, 23, ...     0.0         0.0   \n",
       "2  [7, 15, 164, 122, 8, 6, 935, 4166, 636, 24, 18...     2.4         0.0   \n",
       "3  [133, 106, 26, 749, 16, 35, 94, 24, 903, 2393,...     1.7         0.0   \n",
       "4  [207, 194, 882, 20, 67, 935, 1, 3204, 1281, 15...     1.4         0.7   \n",
       "\n",
       "   colon%  question%  \n",
       "0     0.0        0.0  \n",
       "1     0.0        0.0  \n",
       "2     0.0        0.0  \n",
       "3     0.0        0.0  \n",
       "4     0.0        0.0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"comma%\"] = train.text.apply(lambda x: punct_pct2(x, ','))\n",
    "train[\"semicolon%\"] = train.text.apply(lambda x: punct_pct2(x, ';'))\n",
    "train[\"colon%\"] = train.text.apply(lambda x: punct_pct2(x, ':'))\n",
    "train[\"question%\"] = train.text.apply(lambda x: punct_pct2(x, '?'))\n",
    "\n",
    "test[\"comma%\"] = test.text.apply(lambda x: punct_pct2(x, ','))\n",
    "test[\"semicolon%\"] = test.text.apply(lambda x: punct_pct2(x, ';'))\n",
    "test[\"colon%\"] = test.text.apply(lambda x: punct_pct2(x, ':'))\n",
    "test[\"question%\"] = test.text.apply(lambda x: punct_pct2(x, '?'))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of upper case words in the text ##\n",
    "train[\"num_words_upper\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "test[\"num_words_upper\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of title words in the text ##\n",
    "train[\"num_words_title\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "test[\"num_words_title\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Average length of the words in the text ##\n",
    "train[\"mean_word_len\"] = train[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test[\"mean_word_len\"] = test[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vars = [\"mean_word_len\", \"num_words_title\", \"num_words_upper\", \n",
    "            \"comma%\", \"semicolon%\", \"question%\", \"colon%\",\n",
    "            \"num_punctuations\", \"num_stopwords\", \"num_unique_words\", \"num_words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEMMING WORDS\n",
    "import nltk.stem as stm\n",
    "import re\n",
    "stemmer = stm.SnowballStemmer(\"english\")\n",
    "train[\"stem_text\"] = train.text.apply(lambda x: (\" \").join([stemmer.stem(z) for z in re.sub(\"[^a-zA-Z0-9]\",\" \", x).split(\" \")]))\n",
    "test[\"stem_text\"] = test.text.apply(lambda x: (\" \").join([stemmer.stem(z) for z in re.sub(\"[^a-zA-Z0-9]\",\" \", x).split(\" \")]))\n",
    "\n",
    "#PROCESS TEXT: RAW\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# tok_raw = Tokenizer()\n",
    "# tok_raw.fit_on_texts(train.text.str.lower())\n",
    "tok_stem = Tokenizer()\n",
    "tok_stem.fit_on_texts(train.stem_text.str.lower())\n",
    "train[\"seq_text_stem\"] = tok_stem.texts_to_sequences(train.stem_text)\n",
    "test[\"seq_text_stem\"] = tok_stem.texts_to_sequences(test.stem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXTRACT DATA FOR KERAS MODEL\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def get_keras_data(dataset, maxlen=20, scaler=None, tdfidf=None):\n",
    "    if scaler==None:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(dataset[num_vars])\n",
    "    if tdfidf==None:\n",
    "        tdfidf = Pipeline(steps=[('tdfidf', TfidfVectorizer(analyzer='word', binary=False, ngram_range=(1, 3), stop_words=\"english\")),\n",
    "                                 ('svd', TruncatedSVD(algorithm='randomized', n_components=20, n_iter=10, random_state=None, tol=0.0) )])\n",
    "        tdfidf.fit(dataset.text)\n",
    "    X = {\n",
    "        \"stem_input\": pad_sequences(dataset.seq_text_stem, maxlen=maxlen),\n",
    "        \"num_input\": scaler.transform(dataset[num_vars]),\n",
    "        \"svd_vect\": tdfidf.transform(dataset.text)\n",
    "    }\n",
    "    return X, scaler, tdfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CSY\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\CSY\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing valid...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CSY\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CSY\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n"
     ]
    }
   ],
   "source": [
    "maxlen = 60\n",
    "dtrain, dvalid = train_test_split(train, random_state=1, test_size=0.3)\n",
    "print(\"processing train...\")\n",
    "X_train, scaler, tdfidf = get_keras_data(dtrain, maxlen)\n",
    "y_train = np.array(dtrain[target_vars])\n",
    "print(\"processing valid...\")\n",
    "X_valid, _, _ = get_keras_data(dvalid, maxlen, scaler, tdfidf)\n",
    "y_valid = np.array(dvalid[target_vars])\n",
    "print(\"processing test...\")\n",
    "X_test, _, _ = get_keras_data(test, maxlen, scaler, tdfidf)\n",
    "\n",
    "n_stem_seq = np.max( [np.max(X_valid[\"stem_input\"]), np.max(X_train[\"stem_input\"])])+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "stem_input (InputLayer)         (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 60, 50)       757250      stem_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_13 (SpatialDr (None, 60, 50)       0           embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 3000)         0           spatial_dropout1d_13[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "num_input (InputLayer)          (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "svd_vect (InputLayer)           (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 3031)         0           flatten_13[0][0]                 \n",
      "                                                                 num_input[0][0]                  \n",
      "                                                                 svd_vect[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 256)          776192      concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 256)          0           dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 3)            771         dropout_13[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,534,213\n",
      "Trainable params: 1,534,213\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#KERAS MODEL DEFINITION\n",
    "from keras.layers import Dense, Dropout, Embedding\n",
    "from keras.layers import Flatten, Input, SpatialDropout1D, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam \n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "\n",
    "def get_callbacks(filepath, patience=5):\n",
    "    es = EarlyStopping('val_loss', patience=patience, restore_best_weights=True)\n",
    "    msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "    return [es, msave]\n",
    "\n",
    "def get_model():\n",
    "    embed_dim = 50\n",
    "    dropout_rate = 0.9\n",
    "    emb_dropout_rate = 0.9\n",
    "   \n",
    "    input_text = Input(shape=[maxlen], name=\"stem_input\")\n",
    "    input_num = Input(shape=[X_train[\"num_input\"].shape[1]], name=\"num_input\")\n",
    "    input_svd = Input(shape=[X_train[\"svd_vect\"].shape[1]], name=\"svd_vect\")\n",
    "    \n",
    "    emb_lstm = SpatialDropout1D(emb_dropout_rate) (Embedding(n_stem_seq, embed_dim,input_length = maxlen) (input_text))\n",
    "    concatenate = Concatenate()([(Flatten() (emb_lstm)), input_num, input_svd])\n",
    "    dense = Dropout(dropout_rate) (Dense(256) (concatenate))\n",
    "    \n",
    "    output = Dense(3, activation=\"softmax\")(dense)\n",
    "\n",
    "    model = Model([input_text, input_num, input_svd], output)\n",
    "\n",
    "    optimizer = Adam(lr=0.0008, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0.0)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13705 samples, validate on 5874 samples\n",
      "Epoch 1/150\n",
      "13705/13705 [==============================] - 4s 295us/step - loss: 1.1445 - val_loss: 1.0204\n",
      "Epoch 2/150\n",
      "13705/13705 [==============================] - 2s 182us/step - loss: 1.0753 - val_loss: 1.0014ETA: 1s -\n",
      "Epoch 3/150\n",
      "13705/13705 [==============================] - 3s 183us/step - loss: 1.0413 - val_loss: 0.9885\n",
      "Epoch 4/150\n",
      "13705/13705 [==============================] - 2s 175us/step - loss: 1.0215 - val_loss: 0.9734 ETA: 1s -  - ETA: 0s - loss: \n",
      "Epoch 5/150\n",
      "13705/13705 [==============================] - 2s 176us/step - loss: 1.0030 - val_loss: 0.9532\n",
      "Epoch 6/150\n",
      "13705/13705 [==============================] - 2s 181us/step - loss: 0.9800 - val_loss: 0.9279\n",
      "Epoch 7/150\n",
      "13705/13705 [==============================] - 3s 206us/step - loss: 0.9484 - val_loss: 0.8929\n",
      "Epoch 8/150\n",
      "13705/13705 [==============================] - 3s 191us/step - loss: 0.9110 - val_loss: 0.8476\n",
      "Epoch 9/150\n",
      "13705/13705 [==============================] - 3s 205us/step - loss: 0.8649 - val_loss: 0.7934oss: 0.86\n",
      "Epoch 10/150\n",
      "13705/13705 [==============================] - 3s 224us/step - loss: 0.8275 - val_loss: 0.7378\n",
      "Epoch 11/150\n",
      "13705/13705 [==============================] - 3s 222us/step - loss: 0.7787 - val_loss: 0.6822\n",
      "Epoch 12/150\n",
      "13705/13705 [==============================] - 3s 188us/step - loss: 0.7297 - val_loss: 0.6340\n",
      "Epoch 13/150\n",
      "13705/13705 [==============================] - 3s 192us/step - loss: 0.6860 - val_loss: 0.5929\n",
      "Epoch 14/150\n",
      "13705/13705 [==============================] - 3s 186us/step - loss: 0.6432 - val_loss: 0.5563\n",
      "Epoch 15/150\n",
      "13705/13705 [==============================] - 3s 236us/step - loss: 0.6163 - val_loss: 0.5287\n",
      "Epoch 16/150\n",
      "13705/13705 [==============================] - 3s 206us/step - loss: 0.5850 - val_loss: 0.5071\n",
      "Epoch 17/150\n",
      "13705/13705 [==============================] - 3s 219us/step - loss: 0.5629 - val_loss: 0.4876 0s - loss: 0\n",
      "Epoch 18/150\n",
      "13705/13705 [==============================] - 3s 207us/step - loss: 0.5376 - val_loss: 0.4703\n",
      "Epoch 19/150\n",
      "13705/13705 [==============================] - 3s 203us/step - loss: 0.5113 - val_loss: 0.4583s - loss: 0.5 - ETA: 1s - loss: 0 - ETA: 0s - los\n",
      "Epoch 20/150\n",
      "13705/13705 [==============================] - 3s 202us/step - loss: 0.4913 - val_loss: 0.4477\n",
      "Epoch 21/150\n",
      "13705/13705 [==============================] - 2s 181us/step - loss: 0.4797 - val_loss: 0.4361\n",
      "Epoch 22/150\n",
      "13705/13705 [==============================] - 3s 192us/step - loss: 0.4677 - val_loss: 0.4305\n",
      "Epoch 23/150\n",
      "13705/13705 [==============================] - 3s 189us/step - loss: 0.4517 - val_loss: 0.4225\n",
      "Epoch 24/150\n",
      "13705/13705 [==============================] - 3s 183us/step - loss: 0.4288 - val_loss: 0.4170\n",
      "Epoch 25/150\n",
      "13705/13705 [==============================] - 2s 177us/step - loss: 0.4180 - val_loss: 0.4158\n",
      "Epoch 26/150\n",
      "13705/13705 [==============================] - 3s 199us/step - loss: 0.4029 - val_loss: 0.4114\n",
      "Epoch 27/150\n",
      "13705/13705 [==============================] - 3s 188us/step - loss: 0.3864 - val_loss: 0.4076\n",
      "Epoch 28/150\n",
      "13705/13705 [==============================] - 2s 182us/step - loss: 0.3857 - val_loss: 0.4089\n",
      "Epoch 29/150\n",
      "13705/13705 [==============================] - 2s 177us/step - loss: 0.3713 - val_loss: 0.4055\n",
      "Epoch 30/150\n",
      "13705/13705 [==============================] - 2s 176us/step - loss: 0.3620 - val_loss: 0.4078ss: 0\n",
      "Epoch 31/150\n",
      "13705/13705 [==============================] - 3s 199us/step - loss: 0.3592 - val_loss: 0.4088\n",
      "Epoch 32/150\n",
      "13705/13705 [==============================] - 3s 199us/step - loss: 0.3469 - val_loss: 0.4068 ETA: 0s - loss: 0\n",
      "Epoch 33/150\n",
      "13705/13705 [==============================] - 3s 185us/step - loss: 0.3528 - val_loss: 0.4048 1s - \n",
      "Epoch 34/150\n",
      "13705/13705 [==============================] - 3s 183us/step - loss: 0.3351 - val_loss: 0.4040\n",
      "Epoch 35/150\n",
      "13705/13705 [==============================] - 3s 211us/step - loss: 0.3277 - val_loss: 0.4072\n",
      "Epoch 36/150\n",
      "13705/13705 [==============================] - 3s 222us/step - loss: 0.3140 - val_loss: 0.4075ETA: 1s - loss: 0. - ETA: 0s - loss: 0.3 - ETA: 0s - loss: 0\n",
      "Epoch 37/150\n",
      "13705/13705 [==============================] - 3s 211us/step - loss: 0.3096 - val_loss: 0.4083\n",
      "Epoch 38/150\n",
      "13705/13705 [==============================] - 4s 267us/step - loss: 0.3047 - val_loss: 0.4109\n",
      "Epoch 39/150\n",
      "13705/13705 [==============================] - 3s 219us/step - loss: 0.3019 - val_loss: 0.4126\n",
      "Epoch 40/150\n",
      "13705/13705 [==============================] - 3s 223us/step - loss: 0.3028 - val_loss: 0.4134\n",
      "Epoch 41/150\n",
      "13705/13705 [==============================] - 3s 205us/step - loss: 0.2889 - val_loss: 0.4220\n",
      "Epoch 42/150\n",
      "13705/13705 [==============================] - 3s 194us/step - loss: 0.2902 - val_loss: 0.4221- \n",
      "Epoch 43/150\n",
      "13705/13705 [==============================] - 3s 188us/step - loss: 0.2860 - val_loss: 0.4169\n",
      "Epoch 44/150\n",
      "13705/13705 [==============================] - 3s 188us/step - loss: 0.2790 - val_loss: 0.4208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2475d718278>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TRAIN KERAS MODEL\n",
    "file_path = \".model_weights.hdf5\"\n",
    "callbacks = get_callbacks(filepath=file_path, patience=10)\n",
    "\n",
    "model = get_model()\n",
    "model.fit(X_train, y_train, epochs=150, validation_data=[X_valid, y_valid], batch_size=512, callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12529461447243231\n",
      "0.40399417783453656\n"
     ]
    }
   ],
   "source": [
    "#MODEL EVALUATION\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "model = get_model()\n",
    "model.load_weights(file_path)\n",
    "\n",
    "preds_train = model.predict(X_train)\n",
    "preds_valid = model.predict(X_valid)\n",
    "\n",
    "print(log_loss(y_train, preds_train))\n",
    "print(log_loss(y_valid, preds_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>0.025837</td>\n",
       "      <td>0.002814</td>\n",
       "      <td>0.971349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>0.976705</td>\n",
       "      <td>0.019966</td>\n",
       "      <td>0.003329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>0.013211</td>\n",
       "      <td>0.985711</td>\n",
       "      <td>0.001078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>0.678175</td>\n",
       "      <td>0.312398</td>\n",
       "      <td>0.009427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>0.974057</td>\n",
       "      <td>0.008208</td>\n",
       "      <td>0.017735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       EAP       HPL       MWS\n",
       "0  id02310  0.025837  0.002814  0.971349\n",
       "1  id24541  0.976705  0.019966  0.003329\n",
       "2  id00134  0.013211  0.985711  0.001078\n",
       "3  id27757  0.678175  0.312398  0.009427\n",
       "4  id04081  0.974057  0.008208  0.017735"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PREDICTION\n",
    "preds = pd.DataFrame(model.predict(X_test), columns=target_vars)\n",
    "submission = pd.concat([test[\"id\"],preds], axis=1)\n",
    "submission.to_csv(\"keras.csv\", index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
