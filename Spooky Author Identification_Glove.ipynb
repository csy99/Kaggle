{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, string\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, decomposition, model_selection, metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = preprocessing.LabelEncoder()\n",
    "y = enc.fit_transform(train.author.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct_pct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(['id','author'], axis=1)\n",
    "X[\"len\"] = train.text.apply(lambda x: len(x) - x.count(\" \"))\n",
    "X[\"punct%\"] = train[\"text\"].apply(lambda x: punct_pct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>len</th>\n",
       "      <th>punct%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>191</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>58</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>165</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>173</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>148</td>\n",
       "      <td>2.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  len  punct%\n",
       "0  This process, however, afforded me no means of...  191     3.7\n",
       "1  It never once occurred to me that the fumbling...   58     1.7\n",
       "2  In his left hand was a gold snuff box, from wh...  165     3.0\n",
       "3  How lovely is spring As we looked from Windsor...  173     2.3\n",
       "4  Finding nothing else, not even gold, the Super...  148     2.7"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct_pct2(text, punc):\n",
    "    count = sum([1 for char in text if char==punc])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>len</th>\n",
       "      <th>punct%</th>\n",
       "      <th>comma%</th>\n",
       "      <th>semicolon%</th>\n",
       "      <th>colon%</th>\n",
       "      <th>question%</th>\n",
       "      <th>exclamation%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>191</td>\n",
       "      <td>3.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>58</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>165</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>173</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>148</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  len  punct%  comma%  \\\n",
       "0  This process, however, afforded me no means of...  191     3.7     2.1   \n",
       "1  It never once occurred to me that the fumbling...   58     1.7     0.0   \n",
       "2  In his left hand was a gold snuff box, from wh...  165     3.0     2.4   \n",
       "3  How lovely is spring As we looked from Windsor...  173     2.3     1.7   \n",
       "4  Finding nothing else, not even gold, the Super...  148     2.7     1.4   \n",
       "\n",
       "   semicolon%  colon%  question%  exclamation%  \n",
       "0         1.0     0.0        0.0           0.0  \n",
       "1         0.0     0.0        0.0           0.0  \n",
       "2         0.0     0.0        0.0           0.0  \n",
       "3         0.0     0.0        0.0           0.0  \n",
       "4         0.7     0.0        0.0           0.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[\"comma%\"] = X.text.apply(lambda x: punct_pct2(x, ','))\n",
    "X[\"semicolon%\"] = X.text.apply(lambda x: punct_pct2(x, ';'))\n",
    "X[\"colon%\"] = X.text.apply(lambda x: punct_pct2(x, ':'))\n",
    "X[\"question%\"] = X.text.apply(lambda x: punct_pct2(x, '?'))\n",
    "X[\"exclamation%\"] = X.text.apply(lambda x: punct_pct2(x, '!'))\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len</th>\n",
       "      <th>punct%</th>\n",
       "      <th>comma%</th>\n",
       "      <th>semicolon%</th>\n",
       "      <th>colon%</th>\n",
       "      <th>question%</th>\n",
       "      <th>exclamation%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19579.000000</td>\n",
       "      <td>19579.000000</td>\n",
       "      <td>19579.000000</td>\n",
       "      <td>19579.000000</td>\n",
       "      <td>19579.000000</td>\n",
       "      <td>19579.000000</td>\n",
       "      <td>19579.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>123.326932</td>\n",
       "      <td>3.458471</td>\n",
       "      <td>1.486674</td>\n",
       "      <td>0.177016</td>\n",
       "      <td>0.020251</td>\n",
       "      <td>0.094515</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>87.952007</td>\n",
       "      <td>2.444214</td>\n",
       "      <td>1.313902</td>\n",
       "      <td>0.418833</td>\n",
       "      <td>0.159751</td>\n",
       "      <td>0.506201</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>67.000000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>106.000000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>158.000000</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3803.000000</td>\n",
       "      <td>53.300000</td>\n",
       "      <td>16.700000</td>\n",
       "      <td>7.100000</td>\n",
       "      <td>5.900000</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                len        punct%        comma%    semicolon%        colon%  \\\n",
       "count  19579.000000  19579.000000  19579.000000  19579.000000  19579.000000   \n",
       "mean     123.326932      3.458471      1.486674      0.177016      0.020251   \n",
       "std       87.952007      2.444214      1.313902      0.418833      0.159751   \n",
       "min       17.000000      0.100000      0.000000      0.000000      0.000000   \n",
       "25%       67.000000      2.100000      0.400000      0.000000      0.000000   \n",
       "50%      106.000000      2.900000      1.300000      0.000000      0.000000   \n",
       "75%      158.000000      4.100000      2.200000      0.000000      0.000000   \n",
       "max     3803.000000     53.300000     16.700000      7.100000      5.900000   \n",
       "\n",
       "          question%  exclamation%  \n",
       "count  19579.000000       19579.0  \n",
       "mean       0.094515           0.0  \n",
       "std        0.506201           0.0  \n",
       "min        0.000000           0.0  \n",
       "25%        0.000000           0.0  \n",
       "50%        0.000000           0.0  \n",
       "75%        0.000000           0.0  \n",
       "max        9.400000           0.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>len</th>\n",
       "      <th>punct%</th>\n",
       "      <th>comma%</th>\n",
       "      <th>semicolon%</th>\n",
       "      <th>colon%</th>\n",
       "      <th>question%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>191</td>\n",
       "      <td>3.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>58</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>165</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>173</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>148</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  len  punct%  comma%  \\\n",
       "0  This process, however, afforded me no means of...  191     3.7     2.1   \n",
       "1  It never once occurred to me that the fumbling...   58     1.7     0.0   \n",
       "2  In his left hand was a gold snuff box, from wh...  165     3.0     2.4   \n",
       "3  How lovely is spring As we looked from Windsor...  173     2.3     1.7   \n",
       "4  Finding nothing else, not even gold, the Super...  148     2.7     1.4   \n",
       "\n",
       "   semicolon%  colon%  question%  \n",
       "0         1.0     0.0        0.0  \n",
       "1         0.0     0.0        0.0  \n",
       "2         0.0     0.0        0.0  \n",
       "3         0.0     0.0        0.0  \n",
       "4         0.7     0.0        0.0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.drop(['exclamation%'], axis = 1, inplace=True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19579, 7)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7900\n",
       "2    6044\n",
       "1    5635\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# ensure the % distribution of different author in the training set and validation set after the split\n",
    "xTrain, xVal, yTrain, yVal = train_test_split(X, y, stratify=y, random_state=1, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15663, 7), (3916, 7))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTrain.shape, xVal.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = nltk.WordNetLemmatizer()\n",
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('abandoned', 'abandon')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemmatize(\"abandoned\"), ps.stem(\"abandoned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text = \" \".join([ps.stem(word) for word in tokens if word not in stopwords])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>len</th>\n",
       "      <th>punct%</th>\n",
       "      <th>comma%</th>\n",
       "      <th>semicolon%</th>\n",
       "      <th>colon%</th>\n",
       "      <th>question%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16782</th>\n",
       "      <td>The youth's eyes glistened and his nostrils cu...</td>\n",
       "      <td>107</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>At fust the things didn't never go onto the ma...</td>\n",
       "      <td>76</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>Its persistence among a simple people was quit...</td>\n",
       "      <td>242</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10571</th>\n",
       "      <td>From this time a new spirit of life animated t...</td>\n",
       "      <td>66</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11242</th>\n",
       "      <td>To speak the truth, I had no especial relish f...</td>\n",
       "      <td>275</td>\n",
       "      <td>3.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  len  punct%  comma%  \\\n",
       "16782  The youth's eyes glistened and his nostrils cu...  107     1.9     0.0   \n",
       "1340   At fust the things didn't never go onto the ma...   76     3.9     1.3   \n",
       "435    Its persistence among a simple people was quit...  242     1.2     0.8   \n",
       "10571  From this time a new spirit of life animated t...   66     1.5     0.0   \n",
       "11242  To speak the truth, I had no especial relish f...  275     3.6     2.2   \n",
       "\n",
       "       semicolon%  colon%  question%  \n",
       "16782         0.0     0.0        0.0  \n",
       "1340          0.0     0.0        0.0  \n",
       "435           0.0     0.0        0.0  \n",
       "10571         0.0     0.0        0.0  \n",
       "11242         0.7     0.0        0.0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CSY\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>len</th>\n",
       "      <th>punct%</th>\n",
       "      <th>comma%</th>\n",
       "      <th>semicolon%</th>\n",
       "      <th>colon%</th>\n",
       "      <th>question%</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16782</th>\n",
       "      <td>The youth's eyes glistened and his nostrils cu...</td>\n",
       "      <td>107</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>youth eye glisten nostril curl fume brownish f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>At fust the things didn't never go onto the ma...</td>\n",
       "      <td>76</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>fust thing didnt never go onto main island art...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  len  punct%  comma%  \\\n",
       "16782  The youth's eyes glistened and his nostrils cu...  107     1.9     0.0   \n",
       "1340   At fust the things didn't never go onto the ma...   76     3.9     1.3   \n",
       "\n",
       "       semicolon%  colon%  question%  \\\n",
       "16782         0.0     0.0        0.0   \n",
       "1340          0.0     0.0        0.0   \n",
       "\n",
       "                                              clean_text  \n",
       "16782  youth eye glisten nostril curl fume brownish f...  \n",
       "1340   fust thing didnt never go onto main island art...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTrain[\"clean_text\"] = xTrain['text'].apply(lambda x: clean_text(x))\n",
    "xTrain.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CSY\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>len</th>\n",
       "      <th>punct%</th>\n",
       "      <th>comma%</th>\n",
       "      <th>semicolon%</th>\n",
       "      <th>colon%</th>\n",
       "      <th>question%</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17400</th>\n",
       "      <td>There will be frequent hours in which I shall ...</td>\n",
       "      <td>84</td>\n",
       "      <td>3.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>frequent hour shall need sympathi poetic done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11261</th>\n",
       "      <td>She has now him in hers since, being unaware t...</td>\n",
       "      <td>109</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sinc unawar letter possess proceed exact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  len  punct%  comma%  \\\n",
       "17400  There will be frequent hours in which I shall ...   84     3.6     2.4   \n",
       "11261  She has now him in hers since, being unaware t...  109     2.8     1.8   \n",
       "\n",
       "       semicolon%  colon%  question%  \\\n",
       "17400         0.0     0.0        0.0   \n",
       "11261         0.0     0.0        0.0   \n",
       "\n",
       "                                          clean_text  \n",
       "17400  frequent hour shall need sympathi poetic done  \n",
       "11261       sinc unawar letter possess proceed exact  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xVal[\"clean_text\"] = xVal['text'].apply(lambda x: clean_text(x))\n",
    "xVal.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xtrain_ctv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-13b1753e8ec9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# xtrain_ctv = ctv.transform(xTrain.clean_text)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# xvalid_ctv = ctv.transform(xVal.clean_text)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain_ctv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m# xTrain2= xTrain.reset_index(drop=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# xTrain2.head()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xtrain_ctv' is not defined"
     ]
    }
   ],
   "source": [
    "# # ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}', ngram_range=(1, 3), stop_words = 'english')\n",
    "# ctv = CountVectorizer(ngram_range=(1, 3))\n",
    "\n",
    "# # Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "# ctv.fit(list(xTrain.clean_text) + list(xVal.clean_text))\n",
    "# xtrain_ctv = ctv.transform(xTrain.clean_text) \n",
    "# xvalid_ctv = ctv.transform(xVal.clean_text)\n",
    "# print(xtrain_ctv.shape)\n",
    "# xTrain2= xTrain.reset_index(drop=True)\n",
    "# print(xTrain2.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5547, 9781)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abaft</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abaout</th>\n",
       "      <th>abat</th>\n",
       "      <th>abdic</th>\n",
       "      <th>abdul</th>\n",
       "      <th>abernethi</th>\n",
       "      <th>aberr</th>\n",
       "      <th>abey</th>\n",
       "      <th>abhor</th>\n",
       "      <th>...</th>\n",
       "      <th>zest</th>\n",
       "      <th>zigzag</th>\n",
       "      <th>zit</th>\n",
       "      <th>zobna</th>\n",
       "      <th>zobnarian</th>\n",
       "      <th>zokkar</th>\n",
       "      <th>zone</th>\n",
       "      <th>zuro</th>\n",
       "      <th>æronaut</th>\n",
       "      <th>ærostat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 9781 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abaft  abandon  abaout  abat  abdic  abdul  abernethi  aberr  abey  abhor  \\\n",
       "0      0        0       0     0      0      0          0      0     0      0   \n",
       "1      0        0       0     0      0      0          0      0     0      0   \n",
       "2      0        0       0     0      0      0          0      0     0      0   \n",
       "3      0        0       0     0      0      0          0      0     0      0   \n",
       "4      0        0       0     0      0      0          0      0     0      0   \n",
       "\n",
       "    ...     zest  zigzag  zit  zobna  zobnarian  zokkar  zone  zuro  æronaut  \\\n",
       "0   ...        0       0    0      0          0       0     0     0        0   \n",
       "1   ...        0       0    0      0          0       0     0     0        0   \n",
       "2   ...        0       0    0      0          0       0     0     0        0   \n",
       "3   ...        0       0    0      0          0       0     0     0        0   \n",
       "4   ...        0       0    0      0          0       0     0     0        0   \n",
       "\n",
       "   ærostat  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "\n",
       "[5 rows x 9781 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xtrain_ctv2 = pd.DataFrame(xtrain_ctv.toarray())\n",
    "# xtrain_ctv2.columns = ctv.get_feature_names()\n",
    "# print(xtrain_ctv2.shape)\n",
    "# xtrain_ctv2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5547, 9789)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XTrain = pd.concat([xTrain2, xtrain_ctv2], axis=1)\n",
    "# XTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abaft</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abaout</th>\n",
       "      <th>abat</th>\n",
       "      <th>abdic</th>\n",
       "      <th>abdul</th>\n",
       "      <th>abernethi</th>\n",
       "      <th>aberr</th>\n",
       "      <th>abey</th>\n",
       "      <th>abhor</th>\n",
       "      <th>...</th>\n",
       "      <th>zest</th>\n",
       "      <th>zigzag</th>\n",
       "      <th>zit</th>\n",
       "      <th>zobna</th>\n",
       "      <th>zobnarian</th>\n",
       "      <th>zokkar</th>\n",
       "      <th>zone</th>\n",
       "      <th>zuro</th>\n",
       "      <th>æronaut</th>\n",
       "      <th>ærostat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 9781 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abaft  abandon  abaout  abat  abdic  abdul  abernethi  aberr  abey  abhor  \\\n",
       "0      0        0       0     0      0      0          0      0     0      0   \n",
       "1      0        0       0     0      0      0          0      0     0      0   \n",
       "2      0        0       0     0      0      0          0      0     0      0   \n",
       "3      0        0       0     0      0      0          0      0     0      0   \n",
       "4      0        0       0     0      0      0          0      0     0      0   \n",
       "\n",
       "    ...     zest  zigzag  zit  zobna  zobnarian  zokkar  zone  zuro  æronaut  \\\n",
       "0   ...        0       0    0      0          0       0     0     0        0   \n",
       "1   ...        0       0    0      0          0       0     0     0        0   \n",
       "2   ...        0       0    0      0          0       0     0     0        0   \n",
       "3   ...        0       0    0      0          0       0     0     0        0   \n",
       "4   ...        0       0    0      0          0       0     0     0        0   \n",
       "\n",
       "   ærostat  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "\n",
       "[5 rows x 9781 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xval_ctv2 = pd.DataFrame(xvalid_ctv.toarray())\n",
    "# xval_ctv2.columns = ctv.get_feature_names()\n",
    "# xval_ctv2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(979, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>len</th>\n",
       "      <th>punct%</th>\n",
       "      <th>comma%</th>\n",
       "      <th>semicolon%</th>\n",
       "      <th>colon%</th>\n",
       "      <th>question%</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>But even in this acute moment my chief horror ...</td>\n",
       "      <td>91</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>even acut moment chief horror someth apart imm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She went on, feeling that, if she had paused f...</td>\n",
       "      <td>246</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>went feel paus moment check water miseri would...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A little reading of the 'Dial' will carry you ...</td>\n",
       "      <td>47</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>littl read dial carri great way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sometimes I sat with my eyes fixed on the grou...</td>\n",
       "      <td>118</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sometim sat eye fix ground fear rais lest enco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I now lamented that my great elevation would, ...</td>\n",
       "      <td>94</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>lament great elev would case prevent take accu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  len  punct%  comma%  \\\n",
       "0  But even in this acute moment my chief horror ...   91     1.1     0.0   \n",
       "1  She went on, feeling that, if she had paused f...  246     3.3     2.8   \n",
       "2  A little reading of the 'Dial' will carry you ...   47     6.4     0.0   \n",
       "3  Sometimes I sat with my eyes fixed on the grou...  118     1.7     0.8   \n",
       "4  I now lamented that my great elevation would, ...   94     3.2     2.1   \n",
       "\n",
       "   semicolon%  colon%  question%  \\\n",
       "0         0.0     0.0        0.0   \n",
       "1         0.0     0.0        0.0   \n",
       "2         0.0     0.0        0.0   \n",
       "3         0.0     0.0        0.0   \n",
       "4         0.0     0.0        0.0   \n",
       "\n",
       "                                          clean_text  \n",
       "0  even acut moment chief horror someth apart imm...  \n",
       "1  went feel paus moment check water miseri would...  \n",
       "2                    littl read dial carri great way  \n",
       "3  sometim sat eye fix ground fear rais lest enco...  \n",
       "4  lament great elev would case prevent take accu...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xVal2= xVal.reset_index(drop=True)\n",
    "# print(xVal2.shape)\n",
    "# xVal2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(979, 9789)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Xval = pd.concat([xVal2, xval_ctv2], axis=1)\n",
    "# Xval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XTrain.drop(['text','clean_text'],axis=1,inplace=True)\n",
    "# Xval.drop(['text','clean_text'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [05:19, 6883.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2195892 word vectors.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# load the GloVe vectors in a dictionary:\n",
    "embeddings_index = {}\n",
    "f = open('glove.840B.300d.txt','r', errors = 'ignore', encoding='utf8')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = ''.join(values[:-300])\n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function creates a normalized vector for the whole sentence\n",
    "def sent2vec(s):\n",
    "    from nltk import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    words = str(s).lower()\n",
    "#     words = str(s).lower().decode('utf-8')\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 15663/15663 [00:16<00:00, 947.91it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 3916/3916 [00:03<00:00, 989.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# create sentence vectors using the above function for training and validation set\n",
    "xtrain_glove = [sent2vec(x) for x in tqdm(xTrain.clean_text)]\n",
    "xvalid_glove = [sent2vec(x) for x in tqdm(xVal.clean_text)]\n",
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xvalid_glove = np.array(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scale the data before any neural net:\n",
    "scl = StandardScaler()\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "xvalid_glove_scl = scl.transform(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "# we need to binarize the labels for the neural net\n",
    "ytrain_enc = np_utils.to_categorical(yTrain)\n",
    "yvalid_enc = np_utils.to_categorical(yVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using keras tokenizer here\n",
    "from keras.preprocessing import sequence, text\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 70\n",
    "\n",
    "token.fit_on_texts(list(xTrain.clean_text) + list(xVal.clean_text))\n",
    "xtrain_seq = token.texts_to_sequences(xTrain.clean_text)\n",
    "xvalid_seq = token.texts_to_sequences(xVal.clean_text)\n",
    "\n",
    "# zero pad the sequences\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15663, 70)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,   373,     9,  2861,\n",
       "         4099,  2988,  3824,  5656,  4100,   341,  1015],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,  5657,    10,  2059,    24,   122,\n",
       "        10407,   690,   580,  3108,     5,    50,   395]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(xtrain_pad.shape)\n",
    "xtrain_pad[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider turning this array into dataframe and combining with the punctuation % characteristics mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 15586/15586 [00:00<00:00, 106160.60it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/200\n",
      "15663/15663 [==============================] - ETA: 18:43 - loss: 1.23 - ETA: 10:24 - loss: 1.25 - ETA: 7:38 - loss: 1.2617 - ETA: 6:12 - loss: 1.236 - ETA: 5:23 - loss: 1.230 - ETA: 4:50 - loss: 1.248 - ETA: 4:26 - loss: 1.263 - ETA: 4:06 - loss: 1.256 - ETA: 3:51 - loss: 1.261 - ETA: 3:39 - loss: 1.254 - ETA: 3:29 - loss: 1.257 - ETA: 3:20 - loss: 1.260 - ETA: 3:14 - loss: 1.247 - ETA: 3:07 - loss: 1.233 - ETA: 3:02 - loss: 1.234 - ETA: 2:57 - loss: 1.233 - ETA: 2:52 - loss: 1.227 - ETA: 2:49 - loss: 1.222 - ETA: 2:45 - loss: 1.219 - ETA: 2:42 - loss: 1.217 - ETA: 2:39 - loss: 1.218 - ETA: 2:36 - loss: 1.213 - ETA: 2:33 - loss: 1.210 - ETA: 2:31 - loss: 1.206 - ETA: 2:29 - loss: 1.209 - ETA: 2:27 - loss: 1.206 - ETA: 2:25 - loss: 1.205 - ETA: 2:23 - loss: 1.204 - ETA: 2:21 - loss: 1.205 - ETA: 2:20 - loss: 1.202 - ETA: 2:18 - loss: 1.199 - ETA: 2:16 - loss: 1.199 - ETA: 2:15 - loss: 1.197 - ETA: 2:14 - loss: 1.198 - ETA: 2:12 - loss: 1.199 - ETA: 2:11 - loss: 1.198 - ETA: 2:10 - loss: 1.197 - ETA: 2:09 - loss: 1.201 - ETA: 2:07 - loss: 1.200 - ETA: 2:06 - loss: 1.199 - ETA: 2:05 - loss: 1.198 - ETA: 2:04 - loss: 1.197 - ETA: 2:03 - loss: 1.197 - ETA: 2:02 - loss: 1.197 - ETA: 2:01 - loss: 1.194 - ETA: 2:00 - loss: 1.194 - ETA: 1:59 - loss: 1.192 - ETA: 1:58 - loss: 1.192 - ETA: 1:57 - loss: 1.188 - ETA: 1:56 - loss: 1.187 - ETA: 1:55 - loss: 1.188 - ETA: 1:54 - loss: 1.188 - ETA: 1:53 - loss: 1.187 - ETA: 1:52 - loss: 1.187 - ETA: 1:52 - loss: 1.185 - ETA: 1:51 - loss: 1.184 - ETA: 1:50 - loss: 1.184 - ETA: 1:49 - loss: 1.183 - ETA: 1:48 - loss: 1.183 - ETA: 1:48 - loss: 1.184 - ETA: 1:47 - loss: 1.183 - ETA: 1:46 - loss: 1.183 - ETA: 1:45 - loss: 1.181 - ETA: 1:44 - loss: 1.181 - ETA: 1:44 - loss: 1.181 - ETA: 1:43 - loss: 1.181 - ETA: 1:42 - loss: 1.179 - ETA: 1:41 - loss: 1.179 - ETA: 1:41 - loss: 1.177 - ETA: 1:40 - loss: 1.177 - ETA: 1:39 - loss: 1.177 - ETA: 1:38 - loss: 1.175 - ETA: 1:38 - loss: 1.175 - ETA: 1:37 - loss: 1.174 - ETA: 1:36 - loss: 1.174 - ETA: 1:36 - loss: 1.173 - ETA: 1:35 - loss: 1.172 - ETA: 1:34 - loss: 1.173 - ETA: 1:34 - loss: 1.171 - ETA: 1:33 - loss: 1.171 - ETA: 1:32 - loss: 1.170 - ETA: 1:32 - loss: 1.169 - ETA: 1:31 - loss: 1.168 - ETA: 1:30 - loss: 1.168 - ETA: 1:30 - loss: 1.167 - ETA: 1:29 - loss: 1.166 - ETA: 1:28 - loss: 1.166 - ETA: 1:28 - loss: 1.166 - ETA: 1:27 - loss: 1.166 - ETA: 1:26 - loss: 1.165 - ETA: 1:26 - loss: 1.165 - ETA: 1:25 - loss: 1.164 - ETA: 1:24 - loss: 1.164 - ETA: 1:24 - loss: 1.164 - ETA: 1:23 - loss: 1.163 - ETA: 1:23 - loss: 1.163 - ETA: 1:22 - loss: 1.163 - ETA: 1:21 - loss: 1.163 - ETA: 1:21 - loss: 1.162 - ETA: 1:20 - loss: 1.162 - ETA: 1:20 - loss: 1.161 - ETA: 1:19 - loss: 1.161 - ETA: 1:18 - loss: 1.161 - ETA: 1:18 - loss: 1.160 - ETA: 1:17 - loss: 1.159 - ETA: 1:16 - loss: 1.160 - ETA: 1:16 - loss: 1.159 - ETA: 1:15 - loss: 1.158 - ETA: 1:15 - loss: 1.157 - ETA: 1:14 - loss: 1.158 - ETA: 1:13 - loss: 1.157 - ETA: 1:13 - loss: 1.157 - ETA: 1:12 - loss: 1.157 - ETA: 1:12 - loss: 1.156 - ETA: 1:11 - loss: 1.156 - ETA: 1:10 - loss: 1.155 - ETA: 1:10 - loss: 1.155 - ETA: 1:09 - loss: 1.155 - ETA: 1:09 - loss: 1.154 - ETA: 1:08 - loss: 1.154 - ETA: 1:08 - loss: 1.154 - ETA: 1:07 - loss: 1.154 - ETA: 1:06 - loss: 1.154 - ETA: 1:06 - loss: 1.153 - ETA: 1:05 - loss: 1.153 - ETA: 1:05 - loss: 1.152 - ETA: 1:04 - loss: 1.152 - ETA: 1:03 - loss: 1.151 - ETA: 1:03 - loss: 1.151 - ETA: 1:02 - loss: 1.150 - ETA: 1:02 - loss: 1.150 - ETA: 1:01 - loss: 1.149 - ETA: 1:01 - loss: 1.149 - ETA: 1:00 - loss: 1.149 - ETA: 1:00 - loss: 1.149 - ETA: 59s - loss: 1.149 - ETA: 58s - loss: 1.14 - ETA: 58s - loss: 1.14 - ETA: 57s - loss: 1.14 - ETA: 57s - loss: 1.14 - ETA: 56s - loss: 1.14 - ETA: 56s - loss: 1.14 - ETA: 55s - loss: 1.14 - ETA: 55s - loss: 1.14 - ETA: 54s - loss: 1.14 - ETA: 53s - loss: 1.14 - ETA: 53s - loss: 1.14 - ETA: 52s - loss: 1.14 - ETA: 52s - loss: 1.14 - ETA: 51s - loss: 1.14 - ETA: 51s - loss: 1.14 - ETA: 50s - loss: 1.14 - ETA: 49s - loss: 1.14 - ETA: 49s - loss: 1.14 - ETA: 48s - loss: 1.14 - ETA: 48s - loss: 1.14 - ETA: 47s - loss: 1.14 - ETA: 47s - loss: 1.14 - ETA: 46s - loss: 1.14 - ETA: 46s - loss: 1.14 - ETA: 45s - loss: 1.14 - ETA: 44s - loss: 1.14 - ETA: 44s - loss: 1.14 - ETA: 43s - loss: 1.14 - ETA: 43s - loss: 1.14 - ETA: 42s - loss: 1.14 - ETA: 42s - loss: 1.14 - ETA: 41s - loss: 1.14 - ETA: 41s - loss: 1.13 - ETA: 40s - loss: 1.13 - ETA: 39s - loss: 1.13 - ETA: 39s - loss: 1.13 - ETA: 38s - loss: 1.13 - ETA: 38s - loss: 1.13 - ETA: 38s - loss: 1.13 - ETA: 37s - loss: 1.13 - ETA: 37s - loss: 1.13 - ETA: 36s - loss: 1.13 - ETA: 36s - loss: 1.13 - ETA: 35s - loss: 1.13 - ETA: 35s - loss: 1.13 - ETA: 34s - loss: 1.13 - ETA: 34s - loss: 1.13 - ETA: 33s - loss: 1.13 - ETA: 32s - loss: 1.13 - ETA: 32s - loss: 1.13 - ETA: 31s - loss: 1.13 - ETA: 31s - loss: 1.13 - ETA: 30s - loss: 1.13 - ETA: 30s - loss: 1.13 - ETA: 29s - loss: 1.13 - ETA: 29s - loss: 1.13 - ETA: 28s - loss: 1.13 - ETA: 27s - loss: 1.13 - ETA: 27s - loss: 1.13 - ETA: 26s - loss: 1.13 - ETA: 26s - loss: 1.13 - ETA: 25s - loss: 1.13 - ETA: 25s - loss: 1.13 - ETA: 24s - loss: 1.13 - ETA: 24s - loss: 1.13 - ETA: 23s - loss: 1.12 - ETA: 22s - loss: 1.12 - ETA: 22s - loss: 1.12 - ETA: 21s - loss: 1.12 - ETA: 21s - loss: 1.12 - ETA: 20s - loss: 1.12 - ETA: 20s - loss: 1.12 - ETA: 19s - loss: 1.12 - ETA: 19s - loss: 1.12 - ETA: 18s - loss: 1.12 - ETA: 17s - loss: 1.12 - ETA: 17s - loss: 1.12 - ETA: 16s - loss: 1.12 - ETA: 16s - loss: 1.12 - ETA: 15s - loss: 1.12 - ETA: 15s - loss: 1.12 - ETA: 14s - loss: 1.12 - ETA: 14s - loss: 1.12 - ETA: 13s - loss: 1.12 - ETA: 12s - loss: 1.12 - ETA: 12s - loss: 1.12 - ETA: 11s - loss: 1.12 - ETA: 11s - loss: 1.12 - ETA: 10s - loss: 1.12 - ETA: 10s - loss: 1.12 - ETA: 9s - loss: 1.1214 - ETA: 9s - loss: 1.121 - ETA: 8s - loss: 1.120 - ETA: 8s - loss: 1.120 - ETA: 7s - loss: 1.120 - ETA: 6s - loss: 1.120 - ETA: 6s - loss: 1.119 - ETA: 5s - loss: 1.119 - ETA: 5s - loss: 1.119 - ETA: 4s - loss: 1.118 - ETA: 4s - loss: 1.118 - ETA: 3s - loss: 1.116 - ETA: 3s - loss: 1.116 - ETA: 2s - loss: 1.116 - ETA: 2s - loss: 1.116 - ETA: 1s - loss: 1.115 - ETA: 0s - loss: 1.115 - ETA: 0s - loss: 1.115 - 143s 9ms/step - loss: 1.1146 - val_loss: 0.9628\n",
      "Epoch 2/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - ETA: 2:02 - loss: 1.130 - ETA: 2:05 - loss: 1.197 - ETA: 2:02 - loss: 1.104 - ETA: 2:03 - loss: 1.100 - ETA: 2:02 - loss: 1.091 - ETA: 2:01 - loss: 1.096 - ETA: 2:01 - loss: 1.076 - ETA: 2:02 - loss: 1.073 - ETA: 2:02 - loss: 1.078 - ETA: 2:08 - loss: 1.076 - ETA: 2:08 - loss: 1.066 - ETA: 2:06 - loss: 1.059 - ETA: 2:05 - loss: 1.053 - ETA: 2:04 - loss: 1.053 - ETA: 2:03 - loss: 1.046 - ETA: 2:02 - loss: 1.044 - ETA: 2:04 - loss: 1.044 - ETA: 2:06 - loss: 1.041 - ETA: 2:04 - loss: 1.037 - ETA: 2:03 - loss: 1.030 - ETA: 2:02 - loss: 1.025 - ETA: 2:01 - loss: 1.024 - ETA: 2:01 - loss: 1.024 - ETA: 2:00 - loss: 1.021 - ETA: 1:59 - loss: 1.023 - ETA: 1:58 - loss: 1.028 - ETA: 1:57 - loss: 1.024 - ETA: 1:57 - loss: 1.027 - ETA: 1:56 - loss: 1.025 - ETA: 1:56 - loss: 1.025 - ETA: 1:55 - loss: 1.024 - ETA: 1:54 - loss: 1.021 - ETA: 1:53 - loss: 1.020 - ETA: 1:52 - loss: 1.021 - ETA: 1:52 - loss: 1.020 - ETA: 1:51 - loss: 1.024 - ETA: 1:51 - loss: 1.023 - ETA: 1:50 - loss: 1.022 - ETA: 1:50 - loss: 1.022 - ETA: 1:49 - loss: 1.024 - ETA: 1:48 - loss: 1.024 - ETA: 1:48 - loss: 1.025 - ETA: 1:47 - loss: 1.021 - ETA: 1:46 - loss: 1.022 - ETA: 1:46 - loss: 1.023 - ETA: 1:45 - loss: 1.025 - ETA: 1:45 - loss: 1.024 - ETA: 1:45 - loss: 1.023 - ETA: 1:57 - loss: 1.024 - ETA: 1:59 - loss: 1.022 - ETA: 1:59 - loss: 1.021 - ETA: 1:59 - loss: 1.022 - ETA: 2:00 - loss: 1.025 - ETA: 2:00 - loss: 1.024 - ETA: 1:59 - loss: 1.024 - ETA: 2:00 - loss: 1.021 - ETA: 2:00 - loss: 1.022 - ETA: 1:59 - loss: 1.022 - ETA: 1:59 - loss: 1.022 - ETA: 1:58 - loss: 1.023 - ETA: 1:58 - loss: 1.022 - ETA: 1:57 - loss: 1.021 - ETA: 1:57 - loss: 1.020 - ETA: 1:56 - loss: 1.020 - ETA: 1:56 - loss: 1.021 - ETA: 1:55 - loss: 1.023 - ETA: 1:54 - loss: 1.022 - ETA: 1:53 - loss: 1.021 - ETA: 1:52 - loss: 1.020 - ETA: 1:51 - loss: 1.019 - ETA: 1:50 - loss: 1.021 - ETA: 1:49 - loss: 1.021 - ETA: 1:48 - loss: 1.021 - ETA: 1:47 - loss: 1.020 - ETA: 1:47 - loss: 1.020 - ETA: 1:46 - loss: 1.020 - ETA: 1:45 - loss: 1.020 - ETA: 1:44 - loss: 1.021 - ETA: 1:43 - loss: 1.021 - ETA: 1:42 - loss: 1.020 - ETA: 1:41 - loss: 1.020 - ETA: 1:41 - loss: 1.019 - ETA: 1:40 - loss: 1.019 - ETA: 1:39 - loss: 1.017 - ETA: 1:38 - loss: 1.017 - ETA: 1:37 - loss: 1.017 - ETA: 1:37 - loss: 1.019 - ETA: 1:37 - loss: 1.019 - ETA: 1:40 - loss: 1.020 - ETA: 1:39 - loss: 1.021 - ETA: 1:38 - loss: 1.021 - ETA: 1:37 - loss: 1.021 - ETA: 1:37 - loss: 1.020 - ETA: 1:36 - loss: 1.021 - ETA: 1:35 - loss: 1.021 - ETA: 1:34 - loss: 1.020 - ETA: 1:33 - loss: 1.020 - ETA: 1:33 - loss: 1.018 - ETA: 1:32 - loss: 1.020 - ETA: 1:31 - loss: 1.021 - ETA: 1:30 - loss: 1.021 - ETA: 1:29 - loss: 1.022 - ETA: 1:29 - loss: 1.021 - ETA: 1:28 - loss: 1.021 - ETA: 1:27 - loss: 1.021 - ETA: 1:26 - loss: 1.021 - ETA: 1:26 - loss: 1.021 - ETA: 1:25 - loss: 1.020 - ETA: 1:24 - loss: 1.019 - ETA: 1:23 - loss: 1.018 - ETA: 1:23 - loss: 1.018 - ETA: 1:22 - loss: 1.017 - ETA: 1:21 - loss: 1.017 - ETA: 1:20 - loss: 1.017 - ETA: 1:20 - loss: 1.017 - ETA: 1:19 - loss: 1.019 - ETA: 1:18 - loss: 1.019 - ETA: 1:17 - loss: 1.019 - ETA: 1:17 - loss: 1.018 - ETA: 1:16 - loss: 1.019 - ETA: 1:15 - loss: 1.018 - ETA: 1:15 - loss: 1.018 - ETA: 1:14 - loss: 1.018 - ETA: 1:13 - loss: 1.018 - ETA: 1:12 - loss: 1.019 - ETA: 1:12 - loss: 1.020 - ETA: 1:11 - loss: 1.020 - ETA: 1:10 - loss: 1.019 - ETA: 1:10 - loss: 1.019 - ETA: 1:09 - loss: 1.019 - ETA: 1:08 - loss: 1.019 - ETA: 1:08 - loss: 1.018 - ETA: 1:07 - loss: 1.018 - ETA: 1:06 - loss: 1.017 - ETA: 1:06 - loss: 1.017 - ETA: 1:05 - loss: 1.016 - ETA: 1:04 - loss: 1.016 - ETA: 1:04 - loss: 1.015 - ETA: 1:03 - loss: 1.015 - ETA: 1:02 - loss: 1.014 - ETA: 1:02 - loss: 1.015 - ETA: 1:01 - loss: 1.015 - ETA: 1:00 - loss: 1.015 - ETA: 1:00 - loss: 1.015 - ETA: 59s - loss: 1.016 - ETA: 58s - loss: 1.01 - ETA: 58s - loss: 1.01 - ETA: 57s - loss: 1.01 - ETA: 56s - loss: 1.01 - ETA: 56s - loss: 1.01 - ETA: 55s - loss: 1.01 - ETA: 54s - loss: 1.01 - ETA: 54s - loss: 1.01 - ETA: 53s - loss: 1.01 - ETA: 53s - loss: 1.01 - ETA: 52s - loss: 1.01 - ETA: 51s - loss: 1.01 - ETA: 51s - loss: 1.01 - ETA: 50s - loss: 1.01 - ETA: 49s - loss: 1.01 - ETA: 49s - loss: 1.01 - ETA: 48s - loss: 1.01 - ETA: 48s - loss: 1.01 - ETA: 47s - loss: 1.01 - ETA: 46s - loss: 1.01 - ETA: 46s - loss: 1.01 - ETA: 45s - loss: 1.01 - ETA: 45s - loss: 1.01 - ETA: 44s - loss: 1.01 - ETA: 43s - loss: 1.01 - ETA: 43s - loss: 1.01 - ETA: 42s - loss: 1.01 - ETA: 42s - loss: 1.01 - ETA: 41s - loss: 1.01 - ETA: 40s - loss: 1.01 - ETA: 40s - loss: 1.01 - ETA: 39s - loss: 1.01 - ETA: 39s - loss: 1.01 - ETA: 38s - loss: 1.01 - ETA: 37s - loss: 1.01 - ETA: 37s - loss: 1.01 - ETA: 36s - loss: 1.01 - ETA: 36s - loss: 1.01 - ETA: 35s - loss: 1.01 - ETA: 34s - loss: 1.01 - ETA: 34s - loss: 1.01 - ETA: 33s - loss: 1.01 - ETA: 33s - loss: 1.01 - ETA: 32s - loss: 1.01 - ETA: 31s - loss: 1.01 - ETA: 31s - loss: 1.01 - ETA: 30s - loss: 1.01 - ETA: 30s - loss: 1.01 - ETA: 29s - loss: 1.01 - ETA: 28s - loss: 1.01 - ETA: 28s - loss: 1.01 - ETA: 27s - loss: 1.01 - ETA: 27s - loss: 1.01 - ETA: 26s - loss: 1.01 - ETA: 25s - loss: 1.01 - ETA: 25s - loss: 1.01 - ETA: 24s - loss: 1.01 - ETA: 24s - loss: 1.01 - ETA: 23s - loss: 1.01 - ETA: 22s - loss: 1.01 - ETA: 22s - loss: 1.01 - ETA: 21s - loss: 1.01 - ETA: 21s - loss: 1.01 - ETA: 20s - loss: 1.01 - ETA: 20s - loss: 1.01 - ETA: 19s - loss: 1.01 - ETA: 18s - loss: 1.01 - ETA: 18s - loss: 1.01 - ETA: 17s - loss: 1.01 - ETA: 17s - loss: 1.01 - ETA: 16s - loss: 1.01 - ETA: 15s - loss: 1.01 - ETA: 15s - loss: 1.01 - ETA: 14s - loss: 1.01 - ETA: 14s - loss: 1.01 - ETA: 13s - loss: 1.01 - ETA: 13s - loss: 1.01 - ETA: 12s - loss: 1.01 - ETA: 11s - loss: 1.01 - ETA: 11s - loss: 1.01 - ETA: 10s - loss: 1.01 - ETA: 10s - loss: 1.01 - ETA: 9s - loss: 1.0119 - ETA: 8s - loss: 1.011 - ETA: 8s - loss: 1.011 - ETA: 7s - loss: 1.012 - ETA: 7s - loss: 1.011 - ETA: 6s - loss: 1.011 - ETA: 6s - loss: 1.011 - ETA: 5s - loss: 1.011 - ETA: 4s - loss: 1.011 - ETA: 4s - loss: 1.011 - ETA: 3s - loss: 1.011 - ETA: 3s - loss: 1.011 - ETA: 2s - loss: 1.011 - ETA: 2s - loss: 1.011 - ETA: 1s - loss: 1.011 - ETA: 0s - loss: 1.010 - ETA: 0s - loss: 1.010 - 148s 9ms/step - loss: 1.0098 - val_loss: 0.9219\n",
      "Epoch 3/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - ETA: 2:10 - loss: 0.967 - ETA: 2:07 - loss: 0.977 - ETA: 2:04 - loss: 0.937 - ETA: 2:03 - loss: 0.962 - ETA: 2:02 - loss: 0.981 - ETA: 2:02 - loss: 0.980 - ETA: 2:01 - loss: 0.981 - ETA: 2:00 - loss: 0.988 - ETA: 2:00 - loss: 0.978 - ETA: 2:00 - loss: 0.977 - ETA: 2:01 - loss: 0.971 - ETA: 2:00 - loss: 0.965 - ETA: 1:59 - loss: 0.970 - ETA: 1:58 - loss: 0.973 - ETA: 1:58 - loss: 0.982 - ETA: 1:57 - loss: 0.984 - ETA: 1:57 - loss: 0.986 - ETA: 1:56 - loss: 0.985 - ETA: 1:55 - loss: 0.995 - ETA: 1:56 - loss: 0.993 - ETA: 1:55 - loss: 0.990 - ETA: 1:55 - loss: 0.991 - ETA: 1:54 - loss: 0.989 - ETA: 1:53 - loss: 0.993 - ETA: 1:53 - loss: 0.991 - ETA: 1:52 - loss: 0.988 - ETA: 1:52 - loss: 0.983 - ETA: 1:51 - loss: 0.983 - ETA: 1:51 - loss: 0.982 - ETA: 1:50 - loss: 0.981 - ETA: 1:50 - loss: 0.979 - ETA: 1:50 - loss: 0.976 - ETA: 1:49 - loss: 0.975 - ETA: 1:49 - loss: 0.974 - ETA: 1:48 - loss: 0.971 - ETA: 1:48 - loss: 0.970 - ETA: 1:47 - loss: 0.967 - ETA: 1:46 - loss: 0.968 - ETA: 1:46 - loss: 0.974 - ETA: 1:46 - loss: 0.975 - ETA: 1:45 - loss: 0.973 - ETA: 1:45 - loss: 0.976 - ETA: 1:44 - loss: 0.975 - ETA: 1:44 - loss: 0.977 - ETA: 1:43 - loss: 0.977 - ETA: 1:43 - loss: 0.976 - ETA: 1:42 - loss: 0.975 - ETA: 1:42 - loss: 0.977 - ETA: 1:41 - loss: 0.979 - ETA: 1:41 - loss: 0.978 - ETA: 1:40 - loss: 0.978 - ETA: 1:40 - loss: 0.976 - ETA: 1:39 - loss: 0.977 - ETA: 1:39 - loss: 0.978 - ETA: 1:38 - loss: 0.979 - ETA: 1:38 - loss: 0.980 - ETA: 1:37 - loss: 0.980 - ETA: 1:36 - loss: 0.979 - ETA: 1:36 - loss: 0.979 - ETA: 1:35 - loss: 0.977 - ETA: 1:35 - loss: 0.977 - ETA: 1:35 - loss: 0.977 - ETA: 1:34 - loss: 0.976 - ETA: 1:34 - loss: 0.977 - ETA: 1:33 - loss: 0.978 - ETA: 1:33 - loss: 0.978 - ETA: 1:32 - loss: 0.980 - ETA: 1:32 - loss: 0.982 - ETA: 1:31 - loss: 0.982 - ETA: 1:31 - loss: 0.981 - ETA: 1:30 - loss: 0.981 - ETA: 1:30 - loss: 0.980 - ETA: 1:29 - loss: 0.978 - ETA: 1:29 - loss: 0.978 - ETA: 1:28 - loss: 0.980 - ETA: 1:28 - loss: 0.979 - ETA: 1:27 - loss: 0.980 - ETA: 1:27 - loss: 0.979 - ETA: 1:26 - loss: 0.979 - ETA: 1:26 - loss: 0.978 - ETA: 1:25 - loss: 0.978 - ETA: 1:25 - loss: 0.978 - ETA: 1:24 - loss: 0.979 - ETA: 1:24 - loss: 0.978 - ETA: 1:23 - loss: 0.979 - ETA: 1:23 - loss: 0.979 - ETA: 1:22 - loss: 0.979 - ETA: 1:22 - loss: 0.979 - ETA: 1:21 - loss: 0.979 - ETA: 1:21 - loss: 0.979 - ETA: 1:20 - loss: 0.979 - ETA: 1:20 - loss: 0.978 - ETA: 1:19 - loss: 0.977 - ETA: 1:19 - loss: 0.979 - ETA: 1:18 - loss: 0.977 - ETA: 1:18 - loss: 0.977 - ETA: 1:17 - loss: 0.978 - ETA: 1:16 - loss: 0.979 - ETA: 1:16 - loss: 0.978 - ETA: 1:15 - loss: 0.979 - ETA: 1:15 - loss: 0.979 - ETA: 1:14 - loss: 0.978 - ETA: 1:14 - loss: 0.979 - ETA: 1:13 - loss: 0.979 - ETA: 1:13 - loss: 0.979 - ETA: 1:12 - loss: 0.978 - ETA: 1:12 - loss: 0.978 - ETA: 1:11 - loss: 0.976 - ETA: 1:11 - loss: 0.977 - ETA: 1:10 - loss: 0.977 - ETA: 1:10 - loss: 0.976 - ETA: 1:09 - loss: 0.976 - ETA: 1:09 - loss: 0.976 - ETA: 1:08 - loss: 0.975 - ETA: 1:08 - loss: 0.976 - ETA: 1:07 - loss: 0.976 - ETA: 1:07 - loss: 0.977 - ETA: 1:06 - loss: 0.976 - ETA: 1:06 - loss: 0.976 - ETA: 1:05 - loss: 0.976 - ETA: 1:04 - loss: 0.976 - ETA: 1:04 - loss: 0.976 - ETA: 1:03 - loss: 0.976 - ETA: 1:03 - loss: 0.976 - ETA: 1:02 - loss: 0.977 - ETA: 1:02 - loss: 0.977 - ETA: 1:01 - loss: 0.976 - ETA: 1:01 - loss: 0.976 - ETA: 1:00 - loss: 0.977 - ETA: 1:00 - loss: 0.976 - ETA: 59s - loss: 0.975 - ETA: 59s - loss: 0.97 - ETA: 58s - loss: 0.97 - ETA: 58s - loss: 0.97 - ETA: 57s - loss: 0.97 - ETA: 57s - loss: 0.97 - ETA: 56s - loss: 0.97 - ETA: 56s - loss: 0.97 - ETA: 55s - loss: 0.97 - ETA: 55s - loss: 0.97 - ETA: 54s - loss: 0.97 - ETA: 54s - loss: 0.97 - ETA: 53s - loss: 0.97 - ETA: 53s - loss: 0.97 - ETA: 52s - loss: 0.97 - ETA: 51s - loss: 0.97 - ETA: 51s - loss: 0.97 - ETA: 50s - loss: 0.97 - ETA: 50s - loss: 0.97 - ETA: 49s - loss: 0.97 - ETA: 49s - loss: 0.97 - ETA: 48s - loss: 0.97 - ETA: 48s - loss: 0.97 - ETA: 47s - loss: 0.97 - ETA: 47s - loss: 0.97 - ETA: 46s - loss: 0.97 - ETA: 46s - loss: 0.97 - ETA: 45s - loss: 0.97 - ETA: 45s - loss: 0.97 - ETA: 44s - loss: 0.97 - ETA: 43s - loss: 0.97 - ETA: 43s - loss: 0.97 - ETA: 42s - loss: 0.97 - ETA: 42s - loss: 0.97 - ETA: 41s - loss: 0.97 - ETA: 41s - loss: 0.97 - ETA: 40s - loss: 0.97 - ETA: 40s - loss: 0.97 - ETA: 39s - loss: 0.97 - ETA: 39s - loss: 0.97 - ETA: 38s - loss: 0.97 - ETA: 38s - loss: 0.97 - ETA: 37s - loss: 0.97 - ETA: 37s - loss: 0.97 - ETA: 36s - loss: 0.97 - ETA: 36s - loss: 0.97 - ETA: 35s - loss: 0.97 - ETA: 35s - loss: 0.97 - ETA: 34s - loss: 0.97 - ETA: 34s - loss: 0.97 - ETA: 33s - loss: 0.97 - ETA: 33s - loss: 0.97 - ETA: 32s - loss: 0.97 - ETA: 31s - loss: 0.97 - ETA: 31s - loss: 0.97 - ETA: 30s - loss: 0.97 - ETA: 30s - loss: 0.97 - ETA: 29s - loss: 0.97 - ETA: 29s - loss: 0.97 - ETA: 28s - loss: 0.97 - ETA: 28s - loss: 0.97 - ETA: 27s - loss: 0.97 - ETA: 27s - loss: 0.97 - ETA: 26s - loss: 0.97 - ETA: 26s - loss: 0.97 - ETA: 25s - loss: 0.97 - ETA: 25s - loss: 0.97 - ETA: 24s - loss: 0.97 - ETA: 24s - loss: 0.97 - ETA: 23s - loss: 0.97 - ETA: 23s - loss: 0.97 - ETA: 22s - loss: 0.97 - ETA: 21s - loss: 0.97 - ETA: 21s - loss: 0.97 - ETA: 20s - loss: 0.97 - ETA: 20s - loss: 0.97 - ETA: 19s - loss: 0.97 - ETA: 19s - loss: 0.97 - ETA: 18s - loss: 0.97 - ETA: 18s - loss: 0.97 - ETA: 17s - loss: 0.97 - ETA: 17s - loss: 0.97 - ETA: 16s - loss: 0.97 - ETA: 16s - loss: 0.97 - ETA: 15s - loss: 0.97 - ETA: 15s - loss: 0.97 - ETA: 14s - loss: 0.97 - ETA: 14s - loss: 0.97 - ETA: 13s - loss: 0.97 - ETA: 13s - loss: 0.97 - ETA: 12s - loss: 0.97 - ETA: 11s - loss: 0.97 - ETA: 11s - loss: 0.97 - ETA: 10s - loss: 0.97 - ETA: 10s - loss: 0.97 - ETA: 9s - loss: 0.9727 - ETA: 9s - loss: 0.973 - ETA: 8s - loss: 0.973 - ETA: 8s - loss: 0.973 - ETA: 7s - loss: 0.973 - ETA: 7s - loss: 0.973 - ETA: 6s - loss: 0.973 - ETA: 6s - loss: 0.973 - ETA: 5s - loss: 0.973 - ETA: 5s - loss: 0.973 - ETA: 4s - loss: 0.973 - ETA: 4s - loss: 0.973 - ETA: 3s - loss: 0.973 - ETA: 3s - loss: 0.972 - ETA: 2s - loss: 0.972 - ETA: 1s - loss: 0.972 - ETA: 1s - loss: 0.973 - ETA: 0s - loss: 0.973 - ETA: 0s - loss: 0.973 - 142s 9ms/step - loss: 0.9731 - val_loss: 0.8880\n",
      "Epoch 4/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8128/15663 [==============>...............] - ETA: 2:46 - loss: 0.895 - ETA: 2:39 - loss: 0.944 - ETA: 2:42 - loss: 0.916 - ETA: 2:42 - loss: 0.906 - ETA: 2:46 - loss: 0.922 - ETA: 2:44 - loss: 0.940 - ETA: 2:43 - loss: 0.928 - ETA: 2:44 - loss: 0.921 - ETA: 2:43 - loss: 0.917 - ETA: 2:43 - loss: 0.910 - ETA: 2:42 - loss: 0.904 - ETA: 2:42 - loss: 0.907 - ETA: 2:47 - loss: 0.907 - ETA: 2:45 - loss: 0.908 - ETA: 2:45 - loss: 0.909 - ETA: 2:44 - loss: 0.917 - ETA: 2:43 - loss: 0.915 - ETA: 2:43 - loss: 0.922 - ETA: 2:42 - loss: 0.926 - ETA: 2:41 - loss: 0.924 - ETA: 2:40 - loss: 0.924 - ETA: 2:39 - loss: 0.928 - ETA: 2:39 - loss: 0.925 - ETA: 2:38 - loss: 0.919 - ETA: 2:37 - loss: 0.920 - ETA: 2:37 - loss: 0.922 - ETA: 2:36 - loss: 0.922 - ETA: 2:35 - loss: 0.924 - ETA: 2:34 - loss: 0.929 - ETA: 2:33 - loss: 0.928 - ETA: 2:33 - loss: 0.932 - ETA: 2:32 - loss: 0.935 - ETA: 2:32 - loss: 0.938 - ETA: 2:31 - loss: 0.940 - ETA: 2:30 - loss: 0.942 - ETA: 2:29 - loss: 0.946 - ETA: 2:28 - loss: 0.945 - ETA: 2:28 - loss: 0.948 - ETA: 2:27 - loss: 0.947 - ETA: 2:26 - loss: 0.951 - ETA: 2:26 - loss: 0.954 - ETA: 2:25 - loss: 0.954 - ETA: 2:24 - loss: 0.953 - ETA: 2:23 - loss: 0.952 - ETA: 2:23 - loss: 0.953 - ETA: 2:22 - loss: 0.951 - ETA: 2:22 - loss: 0.952 - ETA: 2:21 - loss: 0.951 - ETA: 2:20 - loss: 0.951 - ETA: 2:19 - loss: 0.952 - ETA: 2:18 - loss: 0.952 - ETA: 2:18 - loss: 0.953 - ETA: 2:17 - loss: 0.955 - ETA: 2:16 - loss: 0.955 - ETA: 2:16 - loss: 0.955 - ETA: 2:15 - loss: 0.955 - ETA: 2:14 - loss: 0.955 - ETA: 2:13 - loss: 0.954 - ETA: 2:12 - loss: 0.954 - ETA: 2:12 - loss: 0.955 - ETA: 2:11 - loss: 0.954 - ETA: 2:11 - loss: 0.953 - ETA: 2:10 - loss: 0.953 - ETA: 2:09 - loss: 0.954 - ETA: 2:08 - loss: 0.953 - ETA: 2:07 - loss: 0.952 - ETA: 2:07 - loss: 0.951 - ETA: 2:06 - loss: 0.953 - ETA: 2:06 - loss: 0.952 - ETA: 2:05 - loss: 0.952 - ETA: 2:04 - loss: 0.951 - ETA: 2:03 - loss: 0.951 - ETA: 2:02 - loss: 0.951 - ETA: 2:02 - loss: 0.951 - ETA: 2:01 - loss: 0.954 - ETA: 2:00 - loss: 0.954 - ETA: 1:59 - loss: 0.955 - ETA: 1:59 - loss: 0.954 - ETA: 1:58 - loss: 0.954 - ETA: 1:57 - loss: 0.954 - ETA: 1:56 - loss: 0.954 - ETA: 1:55 - loss: 0.954 - ETA: 1:54 - loss: 0.954 - ETA: 1:54 - loss: 0.955 - ETA: 1:53 - loss: 0.954 - ETA: 1:52 - loss: 0.954 - ETA: 1:51 - loss: 0.953 - ETA: 1:50 - loss: 0.953 - ETA: 1:50 - loss: 0.953 - ETA: 1:49 - loss: 0.954 - ETA: 1:48 - loss: 0.955 - ETA: 1:47 - loss: 0.954 - ETA: 1:47 - loss: 0.953 - ETA: 1:46 - loss: 0.953 - ETA: 1:45 - loss: 0.953 - ETA: 1:44 - loss: 0.954 - ETA: 1:44 - loss: 0.955 - ETA: 1:43 - loss: 0.955 - ETA: 1:42 - loss: 0.956 - ETA: 1:41 - loss: 0.957 - ETA: 1:40 - loss: 0.957 - ETA: 1:40 - loss: 0.958 - ETA: 1:39 - loss: 0.957 - ETA: 1:38 - loss: 0.957 - ETA: 1:38 - loss: 0.957 - ETA: 1:37 - loss: 0.956 - ETA: 1:36 - loss: 0.958 - ETA: 1:35 - loss: 0.957 - ETA: 1:35 - loss: 0.958 - ETA: 1:34 - loss: 0.958 - ETA: 1:33 - loss: 0.959 - ETA: 1:32 - loss: 0.959 - ETA: 1:32 - loss: 0.960 - ETA: 1:31 - loss: 0.960 - ETA: 1:30 - loss: 0.960 - ETA: 1:29 - loss: 0.961 - ETA: 1:29 - loss: 0.961 - ETA: 1:28 - loss: 0.961 - ETA: 1:27 - loss: 0.962 - ETA: 1:26 - loss: 0.962 - ETA: 1:26 - loss: 0.962 - ETA: 1:25 - loss: 0.961 - ETA: 1:24 - loss: 0.961 - ETA: 1:24 - loss: 0.961 - ETA: 1:23 - loss: 0.961 - ETA: 1:22 - loss: 0.961 - ETA: 1:22 - loss: 0.9619"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(GRU(300, dropout=0.5, recurrent_dropout=0.3, return_sequences=True))\n",
    "model.add(GRU(300, dropout=0.5, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, restore_best_weights=True)\n",
    "model.fit(x=xtrain_pad, y=ytrain_enc, validation_data=(xvalid_pad, yvalid_enc),\n",
    "          batch_size=64, epochs=200, verbose=1, callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>still urg leav ireland inquietud impati father...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>fire want fan could readili fan newspap govern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "      <td>broken frail door found two cleanli pick human...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>While I was thinking how I should possibly man...</td>\n",
       "      <td>think possibl manag without one actual tumbl h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>I am not sure to what limit his knowledge may ...</td>\n",
       "      <td>sure limit knowledg may extend</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...   \n",
       "1  id24541  If a fire wanted fanning, it could readily be ...   \n",
       "2  id00134  And when they had broken down the frail door t...   \n",
       "3  id27757  While I was thinking how I should possibly man...   \n",
       "4  id04081  I am not sure to what limit his knowledge may ...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  still urg leav ireland inquietud impati father...  \n",
       "1  fire want fan could readili fan newspap govern...  \n",
       "2  broken frail door found two cleanli pick human...  \n",
       "3  think possibl manag without one actual tumbl h...  \n",
       "4                     sure limit knowledg may extend  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"clean_text\"] = test['text'].apply(lambda x: clean_text(x))\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create sentence vectors for test set \n",
    "# test_glove = np.array([sent2vec(x) for x in tqdm(test.clean_text)])\n",
    "# test_glove_scl = scl.transform(test_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = token.texts_to_sequences(test.clean_text)\n",
    "test_pad = sequence.pad_sequences(test_seq, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3115824 , 0.26607314, 0.4223445 ],\n",
       "       [0.46715587, 0.28526133, 0.2475828 ],\n",
       "       [0.35904002, 0.5713203 , 0.0696396 ],\n",
       "       [0.2935729 , 0.5764069 , 0.1300202 ]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions = model.predict(test_pad)\n",
    "test_predictions[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>0.311582</td>\n",
       "      <td>0.266073</td>\n",
       "      <td>0.422345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>0.467156</td>\n",
       "      <td>0.285261</td>\n",
       "      <td>0.247583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>0.359040</td>\n",
       "      <td>0.571320</td>\n",
       "      <td>0.069640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>0.293573</td>\n",
       "      <td>0.576407</td>\n",
       "      <td>0.130020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>0.623533</td>\n",
       "      <td>0.171619</td>\n",
       "      <td>0.204848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id         0         1         2\n",
       "0  id02310  0.311582  0.266073  0.422345\n",
       "1  id24541  0.467156  0.285261  0.247583\n",
       "2  id00134  0.359040  0.571320  0.069640\n",
       "3  id27757  0.293573  0.576407  0.130020\n",
       "4  id04081  0.623533  0.171619  0.204848"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([test.id,pd.DataFrame(test_predictions)], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns=[\"id\",\"EAP\",\"HPL\",\"MWS\"]\n",
    "data.to_csv(\"submit.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
